---
title: 'Project 1: Statistical Tools'
author: "Gabriela Levenfeld Sabau"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes   
    toc: TRUE              
    toc_float: TRUE
  geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
  mathjax: local
  self_contained: false
subtitle: MS in Statistics for Data Science
bibliography: references.bib
biblio-style: ieee
citation_package: natbib
nocite: '@*'
link-citations: true
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

\pagebreak
# Introduction

## Dataset description

The dataset used for developed this task is available on Kaggle [@kaggle_telco_churn] and it contains information of a Telco company from $7043$ clients, distributed across $21$ different variables including `gender`, `MonthlyCharges`, and `PhoneService`, among others.

In the business sector, undertanding the number of customers gained and lost is crucial for maximazing profitability. Thus, analyzing if a client is thinking about changing its services into another company can help to evaluate and create new company's strategies. This dataset is designed in order to predict customer churn, storagint the final decision of each client.


## Variables description

The information contained in the dataset can be group into different areas:

**Personal information**.

-   `customerID`; A unique identifier for the client.

-   `gender`; The customer's gender, Female or Male.

-   `SeniorCitizen`;  Binary variable indicating if the customer is older than 65 (Yes) or not (No).

-   `Partner`; Binary variable which storage whether the customer has a partner (Yes) or not (No).

-   `Dependents`; Binary variable that indicates if the client lives with more people (Yes) or not (No).

**Services subscribed**.

-   `tenure`; The number of months the customer has been with the company by the end of the third quarter (3Q).

-   `PhoneService`; Whether the customer subscribes to home phone service (Yes or No).

-   `MultipleLines`; Whether the customer has more than one telephone lines (Yes or No). This service is on ly available if the user has active the `PhoneService`.

-   `InternetService`; Type of subscription to Internet service (No, DSL, Fiber Optic, or Cable).

The following extra-services are only available for user who has active the `InternetService` (the company does not charge any additional fee).

-   `OnlineSecurity`; Whether the customer subscribes to an online security service (Yes or No).

-   `OnlineBackup`; Whether the customer subscribes to an online backup service (Yes or No).

-   `DeviceProtection`; Whether the customer subscribes to a device protection plan (Yes or No).

-   `TechSupport`; Whether the customer subscribes to a technical support plan from the company with reduced wait times (Yes or No).

-   `StreamingTV`; Whether the customer use the Internet service to stream television programming (Yes, No).

-   `StreamingMovies`; Whether the customer use the Internet service to stream movies (Yes, No).

**Account information**

-   `Contract`; The type of contract that the customer have signed (Month-to-Month, One Year, Two Year).

-   `PaperlessBilling`; Whether the customer has chosen paperless billing (Yes or No).

-   `PaymentMethod`; Customer payment method (Bank Withdrawal, Credit Card, or Mailed Check)

-   `MonthlyCharges`; Customer monthly charges for all services from the company.

-   `TotalCharges`; Total expenses calculated to the end of the 3Q (third quarter).

**Target variable**

-   `Churn`; Whether the customer left the company (Yes) this quarter or stay with the company (No).

## Goal

The objective of this assignment is to predict the `Churn` variable, which represents whether a client will remain with or leave from the company's services within the coming time period. This prediction task is a binary classification problem where the goal is to forecast one of the two possible outcomes: churn yes (leave the company) or churn no (stay with the company services).

This valuable information can provide insights to the company, that allow it to form better strategies and enhance customer retention by optimizing service offering, as well as increase its profitability.

\pagebreak
# Data preprocessing

This section focuses on preparing the dataset for exploratory data analysis (EDA) and applying the computational models. This include loading the necessary material, handling missing values, encoding categorical variables, and splitting the dataset into training and testing subsets, among other tasks.


## Load material.

**Load libraries**. Several libraries are need for the project in order to work properly.

```{r load_libraries, message=FALSE, warning=FALSE}
library(skimr) # Summary statistics
library(tidyverse) # Include ggplot2, dplyr, among others
library(mice) # Imputation missing data
library(VIM) # Imputation graph
library(caret) # Statistical models
library(GGally) # For plots of the continuous variables
library(pander) # For making tables
library(gridExtra) # For grid.arrange
library(reshape2) # Data reshaping (melt)
library(corrplot) # For correlation matrix
library(patchwork)
library(tidyr)
library(knitr)
library(psych) # Compute the correlation matrix
library(polycor) # Compute the correlation heatmap
library(corrplot) # Visualize the correlation matrix
library(pROC)
```

**Load dataset**. Import the dataset.

```{r load_data}
data <- read.csv("Telco-Customer-Churn.csv", header = TRUE, sep = ",")
```

**First look at the dataset**. Brief overview of the dataset for further preprocessing steps.

```{r small_EDA1, eval=FALSE, include=FALSE}
# Small Exploratory Data Analysis
# skim(data)
# Presents results for every column; the statistics it provides depend on the class of the variable

# For visualizing the first 2 rows
# head(data, 2)
```

```{r small_EDA2}
glimpse(data)
```


## Delete unnecessary information.

Some variables are removed due to its usefulness contribution on the model for making predictions. For instance, `customerID` is just the unique identifier for each customer. On the other hand, `PaperlessBilling` parameter, nowadays does not have too much sense. The main reason is that with the digital era we live in, most customers choose for internet payments. Thus this columns are exclude from the analsys.

```{r rmv_columns}
# Remove unnecessary variables
data <- data %>% dplyr::select(-c(customerID, PaperlessBilling))
```

## Data cleaning.

By analyzing the dataset, we can realized that certain variables are closely related, therefore some modification are apply.

<u>Combine `PhoneService` and `MultipleLines`</u>. The first variable indicates if the client has subscribed to home phone service or not. Only when the client has hired this service, he can choose between having $1$ phone line (`MultipleLines`=No) or have more than $1$ (`MultipleLines`=Yes). This relationship lead us into three possible scenarios:

1. **No phone service**. Customers who have not subscribed to phone service (`PhoneService`=No and `MultipleLines`="No phone service").

2. **Single phone line**. Customers with phone service and a single line (`PhoneService`=Yes and `MultipleLines`=No).

3. **Multiple phone lines**. Customers with phone service and multiple lines (`PhoneService`=Yes and `MultipleLines`=Yes).

To simplify this, it is introduced a new variable (`PhoneServiceType`) with three categories corresponding to the above scenarios:

- `NoPhone`: if there is no phone service.

- `PhoneSingle`: if there is phone service with a single line.

- `PhoneMultiple`: if there is phone service with multiple lines.

<u>`InternetService` and its associate variables</u>.
A similar situation happens with `InternetService` and its related services (`OnlineSecurity`, `StreamingTV`, among others). Clients without internet service cannot sign up to these additional services, which are denoted as "Yes", "No", or "No internet service" in the dataset. We can simplify this by considering "No internet service" as equivalent to "No", thereby converting these variables into binary classes.

**Note**. For both situation, it has been computationally verified that there is consistency between the relationships of these variables. Thus avoiding the existence of possible nonsense situations.

```{r data_cleaning, echo=TRUE, results='hide'}
## Unique values of each variable ----------------------------------------------
cat_vars <- sapply(data, is.character) # Select qualitative variables
lapply(data[, cat_vars], unique) # Print each unique values


## PhoneService and MultipleLines ----------------------------------------------
# Subset with clients where PhoneService is "No"
subset_no_Phone <- data[data$PhoneService == "No", ]
# Check MultipleLines is set to "No phone service"
if (all(subset_no_Phone$MultipleLines == "No phone service")) {
  cat("In the subset where PhoneService is 'No', 
      MultipleLines is set to 'No phone service' only.\n")
} else {
  cat("In the subset where PhoneService is 'No', 
      MultipleLines is not exclusively set to 'No phone service'.\n")
}

# Combine PhoneService and MultipleLines on a new variable (PhoneServiceType)
# Convert into a factor
data <- data %>%
  mutate(PhoneServiceType = case_when(
    MultipleLines == "No phone service" ~ "NoPhone",
    MultipleLines == "No" ~ "PhoneSingle",
    MultipleLines == "Yes" ~ "PhoneMultiple"
  )) %>%
  mutate(PhoneServiceType = factor(PhoneServiceType, levels = c("NoPhone", "PhoneSingle", "PhoneMultiple")))

# Remove original variables
data <- dplyr::select(data, -PhoneService, -MultipleLines)


## Consequences Internet service -----------------------------------------------
# Subset with clients where InternetService is "No"
subset_no_Internet <- data[data$InternetService == "No", ]

vars_to_check <- c("OnlineSecurity", "OnlineBackup", "DeviceProtection",
                   "TechSupport", "StreamingTV", "StreamingMovies")

# Check its associate variables are set to "No internet service"
for (variable in vars_to_check) {
  unique_values <- unique(subset_no_Internet[[variable]])
  cat("Unique values for", variable, ":", unique_values)
  cat("\n")
}

# Reduce to a binary variable ("No internet service" -> "No")
for (variable in vars_to_check) {
  data[[variable]] <- ifelse(data[[variable]] == "No internet service", "No", data[[variable]])
}
```

## Handling missing data.

In the dataset, a small observation of the variable `TotalCharges` contains missing values, only the $0.156$% of the customers. To handle these missing values, we have several options:

- **Delete this rows**; Due to the low presence of missing values, deleting these observation will not affect significantly the dataset.

- **Set NA's to $0$**; Since these observations correspond to new clients we can assume it has not passed enough time to compute this value and stablishing to zero can be a good approach.

- **Imputation**; We can impute the missing data using techniques provided by the R library `mice` [@mice_ref]. By default, mice generates $5$ imputations. However, for this project, we will only consider the first imputation. We will use the 'pmm' method (predictive mean matching), which imputes data based on the mean as a reference. After generating the imputed data, we can complete the missing values using the `complete()` function, which fills in missing values with those from the first imputation.

To check there are no missing data anymore, we use a graph from the `library(VIM)` [@VIM_ref], which display the percentage of NA's.

Also, to assess whether the imputed data adequately reflects the distributions of the original data, we can use the `densityplot()` function. This function shows the marginal distribution of the observed data in blue, and in red, it shows the $5$ densities for the predictor with initially missing data.

```{r counting_NAs, eval=FALSE, include=FALSE, results='hide'}
# Counting NA's ----------------------------------------------------------------
missing_values <- colSums(is.na(data)) # 11 NA's in TotalCharges variable
percentage_na <- missing_values/nrow(data)*100 # NA's in %
sort(percentage_na, decreasing = TRUE)

# Get the index of the rows where TotalCharges=NA
index_TotalCharges_na <- which(is.na(data$TotalCharges))
print(index_TotalCharges_na)
```

```{r handling_NAs, fig.width=4, fig.height=3, fig.align='center'}
## MICE Imputation -------------------------------------------------------------
meth <- rep("", ncol(data)) # Set all methods to "" (no imputation)
names(meth)[names(meth) == "TotalCharges"] <- "pmm" # Specify TotalCharges method

mice_model <- mice(data,
  m = 5, maxit = 5, method = "pmm",
  seed = 1234, print = FALSE
)
data <- complete(mice_model, 1) # Complete the data with the first imputation
```


```{r plots_handling_NAs, echo=FALSE, fig.align='center', fig.height=3, fig.width=4}
# Plot 1: Represent the % of missing data
aggr(data,
  col = c("navyblue", "yellow"), numbers = TRUE, sortVars = FALSE,
  labels = names(data), cex.axis = .7, gap = 3,
  ylab = c("Missing data", "Real data")
)

# Plot 2: Check it follows the distribution of the original variable
densityplot(mice_model, scales = list(x = list(relation = "free")))
```

As we can observed on the density plot, the distribution does not fit as properly as it should be. Hence, setting this observation to $0$ or deleting them, might be a better option. However, due to the low presence of missing values on the dataset, we will continue the project with this imputation technique.


## Encoding categorical variables.

Once we have the dataset ready, the goal of this section is to get a dataset that our statistical models knows how to manage. This include, turn it into factor the categorical variables.

**Binary variables**. There are a total of $11$ binary variables: `gender`, `SeniorCitizen`, `Partner`, `Dependents`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies` and `Churn`. All this variables has been transformed into factor, with two possible outcomus "No" or "Yes".

**Multi-sate variables**. There are $4$ variables of this kind of data: `InternetService`, `Contract`, `PaymentMethod`, and `PhoneServiceType`. As well as before, I will create factor levels for each possible outcome. Notice that this starategy can be follow due to the absence of any kind of "natural order".

```{r encoding_categorical_vars, echo=TRUE, results='hide'}
## Binary encoding -------------------------------------------------------------
binary_variables <- c(
  "Partner", "Dependents", "OnlineSecurity", "OnlineBackup",
  "DeviceProtection", "TechSupport", "StreamingTV", "StreamingMovies", "Churn"
)
for (var in binary_variables) {
  data[[var]] <- factor(data[[var]], levels = c("No", "Yes"))
}

data$gender <- factor(data$gender)
levels(data$gender) # Check the levels

unique(data$SeniorCitizen) # It is already transform in 1's and 0's
data$SeniorCitizen <- factor(data$SeniorCitizen, 
                             levels = c(0, 1), 
                             labels = c("No", "Yes"))
levels(data$SeniorCitizen)

## Multi-state variables -------------------------------------------------------
# Rename levels of all variables needed to avoid misunderstands later

# InternetService to a factor and rename levels
data$InternetService <- factor(data$InternetService,
                               levels = c("DSL", "Fiber optic", "No"),
                               labels = c("DSL", "FiberOptic", "No"))
levels(data$InternetService) # Check the levels


# Contract to a factor and rename levels
data$Contract <- factor(data$Contract,
                        levels = c("Month-to-month", "One year", "Two year"),
                        labels = c("MonthToMonth", "OneYear", "TwoYear"))
levels(data$Contract) # Check the levels


# PaymentMethod to a factor and rename levels
data$PaymentMethod <- factor(data$PaymentMethod,
                             levels = c("Electronic check", 
                                        "Mailed check", 
                                        "Bank transfer (automatic)",
                                        "Credit card (automatic)"),
                             labels = c("ElectronicCheck",
                                        "MailedCheck", 
                                        "BankTransfer",
                                        "CreditCard")
                             )
levels(data$PaymentMethod) # Check the levels
```


## Split the dataset

Final step before applying the statistical tools consist on divide the dataset into two subsets. The training set, takes $80$% of the data and it is used to train the models. This subset allows the model to learn patterns and relationships within the data. On the other hand, testing set ($20$% of the data) is used by unseen data to evaluate the performance of the trained model.

```{r split_dataset, echo=TRUE, results='hide'}
set.seed(1234) # For reproducibility

# Split into training (80%) and testing (20%) set
index <- createDataPartition(data$Churn, p=0.8, list=FALSE)
training <- data[ index,]
testing <- data[-index,]

nrow(training) # Number of observation for training
nrow(testing) # Number of observation for testing
```


\pagebreak
# Exploratory Data Analysis (EDA)

The aim is to uncover patterns, detect outliers, and test assumptions through visual and quantitative methods.

## Numeric variables.

`ggpair` function from the GGally package, allow us to create a scatter plot matrix for this three variables. This matrix includes histograms for each variable along the diagonal, showcasing their distribution. The lower triangle displays scatter plots to explore the relationships between variable pairs, while the upper triangle shows the correlation coefficients, providing a quantitative measure of their relationships. This information allow us to identify the shape distribution, tendencies, outliers, among other things, thanks to different views in one grid.


```{r scatterPlot_contiuous}
# Select continuous variables (include target)
continuous_data <- training[, c("tenure", "MonthlyCharges", "TotalCharges", "Churn")]

# Create scatter plot matrix
ggpairs(continuous_data,
        aes(fill="pink"),
        lower = list(continuous = "points", combo = "box_no_facet"),
        upper = list(continuous = "cor"),
        diag = list(continuous = "barDiag"),
        title = "Scatter plot matrix"
       )
```

-   **Tenure**

The histogram shows a bimodal distribution, with peaks at the extremes of the range. This implies that most of the customers are either new (low peak) or very loyal (high peak), with fewer clients in the mid-range.

-   **MonthlyCharges**

Displays a significant peak at the begging of the distribution, suggesting a big amount of clients that pays around 25\$ per month. On the other hand, the distribution becomes more uniform for charges between $60$\$ and $120$\$.

-   **TotalCharges**

It follows a right skewness distribution, which means most of the people are subscribe to services that require to pay less amount of money. It is clear, that clients are willing to pay no more than $1250 \$$. For trying to achieve a normal distribution I will apply a log transformation on this variable.

**General insights**.

This analysis reveals non-normal distribution for the numeric variables due to the used of some statistical methods, the data will be standardized. In addition, it is worth mentioning the high correlation that exist between `tenure` and `TotalCharges` (correlation coefficient of $0.823$). Finally, it is also notably that this two parameters presents some outliers.

**Relation with the output**.

If we keep doing a deeper exploration, customers with lower total charges are more likely to leave (`Churn`=Yes). Furthermore, the second insight is regarding the tenure. The density plot lead us into the conclusion that clients tends to leave within the first year. This could be because they find better alternatives or due to dissatisfaction with the company.

```{r totalCharges_plot, echo=FALSE}
plot_TotalCaharges <- ggplot(training,
                             aes(x = TotalCharges, fill = factor(Churn))) +
  geom_density(alpha = 0.5) + # Overlay density plots, alpha: transparency
  scale_fill_manual(values = c("#FFD39B", "lightblue1")) +
  labs(x = "Total Charges", y = "Density", fill = "Churn") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), # Center the title
        plot.margin = unit(c(1, 1, 1, 1), "lines")) + # Adjust the margin
  ggtitle("TotalCharges by Churn status")

plot_tenure <- ggplot(training, aes(x = tenure, fill = factor(Churn))) +
  geom_density(alpha = 0.5) + # Overlay density plots, alpha: transparency
  scale_fill_manual(values = c("#FFD39B", "lightblue1")) +
  labs(x = "Tenure (nº of months)", y = "Density", fill = "Churn") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), # Center the title
        plot.margin = unit(c(1, 1, 1, 1), "lines")) + # Adjust the margin
  ggtitle("Tenure by Churn status")

# Arrange plots in a grid
grid.arrange(plot_TotalCaharges, plot_tenure, ncol = 2)
```

**Data transformation code**.

```{r numeric_transf}
# Numeric transformation
# TRAINING dataset -------------------------------------------------------------
training_transf <- training
# Log-transformation to solve TotalCharges right skewness
training_transf$TotalCharges <- log(training_transf$TotalCharges)
# Standardizing the variables
training_transf[c("tenure", "MonthlyCharges", "TotalCharges")] <- scale(training_transf[c("tenure", "MonthlyCharges", "TotalCharges")])

# TESTING dataset --------------------------------------------------------------
testing_transf <- testing
# Log-transformation to solve TotalCharges right skewness
testing_transf$TotalCharges <- log(testing_transf$TotalCharges)
# Standardizing the variables
testing_transf[c("tenure", "MonthlyCharges", "TotalCharges")] <- scale(testing_transf[c("tenure", "MonthlyCharges", "TotalCharges")])
```

## Cualitative variables.

**Binary variables**.

On the below table, we observe that `gender` as well as `Partner` are data equally represented on the dataset, with both categories close to a $50\%$ split. Meanwhile, only approximately $16$% of the observations belong to senior citizens. This information aligns with the demographic expectation.

```{r eda_binary_table, echo=FALSE}
# For binary variables
binary_variables <- c("SeniorCitizen", "Partner", "Dependents", 
                      "OnlineSecurity", "OnlineBackup", "DeviceProtection", 
                      "TechSupport", "StreamingTV", "StreamingMovies")

# Information about data storage on training set -------------------------------
summary_table <- lapply(binary_variables, function(variable) {
  counts <- table(training[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

# Use kable() for a simple HTML table
kable(summary_table, format = "html", escape = FALSE)

# Gender needs to be done separately due to its output values
gender_count <- training %>%
  count(gender)

# Print the gender count table
kable(gender_count, format = "html")
```

The following plots aim to show the churn distribution across each binary variable. We realized that customer who leaves does not differ by gender or whether they have partner or not. It looks like, people who are subscribed to extra-services such as online security, online backup, device protection, and tech support or has dependents, they tend to stay at the company (`Churn`=No) most of the time.

```{r eda_binary_plot, echo=FALSE}
# Including gender
bin_variables <- c("gender", "SeniorCitizen", "Partner", "Dependents",
                   "OnlineSecurity", "OnlineBackup", "DeviceProtection",
                   "TechSupport", "StreamingTV", "StreamingMovies")

# Plots Churn by variable ------------------------------------------------------
plot_churn_distribution <- function(data, variable) {
  plot_data <- data.frame(table(data[[variable]], data$Churn))
 
  ggplot(plot_data, aes(x = Var1, y = Freq, fill = factor(Var2))) +
    geom_bar(stat = "identity",  position = position_dodge()) +
    labs(title = paste("Churn by", variable),
         x = variable, y = "Count", fill = "Churn") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 10),
          plot.margin = margin(10, 10, 10, 10))
}

# Plot the first four variables
first_four_plots <- lapply(bin_variables[1:4], function(variable) {
  plot_churn_distribution(training_transf, variable)
})


# Combine the first four plots
combined_first_four <- wrap_plots(first_four_plots, ncol = 2) +
  plot_layout(guides = 'collect') &
  theme(plot.title = element_text(size = 12))

# Display the first four plots
print(combined_first_four)


# Plot the next six variables
next_six_plots <- lapply(bin_variables[5:10], function(variable) {
  plot_churn_distribution(training_transf, variable)
})

# Combine the next six plots
combined_next_six <- wrap_plots(next_six_plots, ncol = 3) +
  plot_layout(guides = 'collect') &
  theme(plot.title = element_text(size = 12))

# Display the next six plots
print(combined_next_six)
```

**Category variables**.

Next step consist on explore the $4$ multi-state variables: `InternetService`, `Contract`, `PaymentMethod`, and `PhoneServiceType`. Some insights from this plots are that a majority of customers prefer having internet as well as phone services, suggesting they are essential services. However, there is no a favorite payment method, more less equally distributed. Lastly, it looks like clients most popular choice is to revenue the contract month by month.

```{r eda_category_distribution, echo=FALSE}
# InternetService
plot_internetService <- ggplot(training, aes(x = InternetService)) +
  geom_bar(fill = "lightblue1") +
  labs(title = "Distribution of Internet Service Types",
       x = "Internet Service",
       y = "Count") +
  theme_minimal()

# Contract
plot_contract <- ggplot(training, aes(x = Contract)) + 
  geom_bar(fill = "coral") +
  labs(title = "Distribution of Contract Types",
       x = "Contract Type", 
       y = "Count") +
  theme_minimal()

# PaymentMethod
plot_paymentMethod <- ggplot(training, aes(x = PaymentMethod)) + 
  geom_bar(fill = "palegreen") +
  labs(title = "Distribution of Payment Methods", 
       x = "Payment Method",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# PhoneServiceType
plot_phoneServiceType <- ggplot(training, aes(x = as.factor(PhoneServiceType))) +
  geom_bar(fill = "orchid") +
  labs(title = "Distribution of Phone Service Types", 
       x = "Phone Service Type", 
       y = "Count") +
  theme_minimal()

# Arrange plots in a grid
grid.arrange(plot_internetService, plot_contract, plot_paymentMethod, plot_phoneServiceType, ncol = 2)
```

Finally, if we study the relationship of these variables with the target we realized people with two year contracts tends to be loyaler clients (which are less likely to change to another company). While customer paying by electronic check have a higher probability to abandon the company.

```{r eda_category_byChurn, echo=FALSE}
# Melt the training data to long format for faceting
category_data <- melt(training, id.vars = "Churn", measure.vars = c("InternetService", "Contract", "PaymentMethod", "PhoneServiceType"))

# Create the combined plot using the melted data
ggplot(category_data, aes(x = value, fill = Churn)) + 
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("No" = "darkolivegreen3", "Yes" = "brown3")) +
  labs(title = "Relationship with Churn",
       x = "", y = "Proportion") +
  facet_wrap(~variable, scales = "free_x", nrow = 2) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.background = element_blank(),
        strip.text.x = element_text(size = 10))
```


## Correlation matrix

The heatmap represents the strength and direction of the relationships between the variables in the dataset, where the color intensity indicates the magnitude of the correlation. Dark colors, that can be either blue (positive) or red (negative), suggest a stronger relationship between variables. A positive coefficient means that an increase (or decrease) in one variable is associated with an increase (or decrease) in the other. Conversely, a negative correlation indicated that as one variables increase, the other decrease. And, color in the middle are interpreted as weak correlations.

For dealing with mixed data type, the `hetcor`function from the `psych` [@psych_ref] package in R is used. This enable the computation of correlation across numeric, binary and multi-state variables as long as the qualitative variable are represented in factors.

```{r correlation_heatmap}
# Works for mix variables: binay, multi-state and numeric
# Qualitative variables have to be in factor

cor_matrix <- hetcor(training_transf)
cor_values <- cor_matrix$correlations
corrplot(cor_values, method="color", type="upper", order="hclust",
         tl.cex=0.7, tl.col="black", tl.srt=45)
```

**Analytical exploration**

Coefficients higher than $0.7$ or lower than $-0.7$ are considered strong. In this dataset we encounter $5$ strong relationship, that can be shown on the below table.

Most of this relationships are kind of obvious. For instance, if the client spend more time having the company services, then it accumulated charges will be higher too. However, we can get an interesting insight: customers who subscribe to one streaming service are likely to subscribe to the other (`StreamingMovies `-`StreamingTV` = $0.7526$).

```{r correlation_analysis}
cor_flat <- as.data.frame(as.table(cor_values))
cor_flat <- cor_flat[order(-abs(cor_flat$Freq)),]

# Filter correlations: >0.7 or <-0.7
strong_correlations <- subset(cor_flat, abs(Freq) > 0.7 & Var1 != Var2)

# Remove duplicates rows
strong_correlations$pairID <- apply(strong_correlations[, c("Var1", "Var2")], 
                                    1,
                                    function(x) paste(sort(x), collapse = "-"))
strong_correlations <- strong_correlations[!duplicated(strong_correlations$pairID),]
strong_correlations$pairID <- NULL # Remove ID

# Add sign
strong_correlations$Sign <- ifelse(strong_correlations$Freq > 0, "+", "-")

pander(strong_correlations)
```

## Target variable

To see the distribution of our target variable (`Churn`), we first look at the raw counts and then at the percentages to understand the proportion of customers who leave versus those who stay with the company's services.

```{r target_distribution}
# Check counts of Churn
pander(table(data$Churn))

# Results in percentage
pander(prop.table(table(data$Churn)) * 100)
```

The dataset is slightly imbalance in the distribution of the `Churn` variable. There are more observations from clients who decide to stay with the company services (`Churn`=No) than who decide to leave. Although the existence of imbalance on the dataset, it is relatively minor.

After applying a sampling technique in order to solve this issue, no improvement was observed in the model's performance. In fact, the results were worse compared to using the original data proportions. This results suggest that the imbalance may not impact the model's accuracy. Therefore, I have decided not to apply any methods for balancing the dataset when computing the statistical tools. However, code for performing upsampling is provided in the [Annex section](#annex).


\pagebreak
# Statistical tools

The objective of this section is to evaluate the performance of some classification models for the binary classification task. The goal is to predict whether the customer intends to stop using the company's services or not. For each of these techniques, we will compute the confusion matrix, a tool which will indicate us how many instances were correctly classified and how many were misclassified.

First, the dataset is divided into predictors and the target variable.

```{r split_subsets}
# Divide into predictors and target both subsets
X_train <- training_transf %>% dplyr::select(-Churn)
y_train <- training_transf$Churn

X_test <- testing_transf %>% dplyr::select(-Churn)
y_test <- testing_transf$Churn
n_test <- dim(testing_transf)[1]
```

To ensure a robust evaluation, a common training control parameter is configure: $5$ repeats of $10$-fold cross-validation.

```{r setUp_trainControl}
# Set up train control -> 5 repeats of 10-fold cross validation
train_ctrl <- trainControl(method = "repeatedcv",
                           repeats = 5,
                           number = 10)
```

Finally, it is worth mentioning that we generated a plot for all models with the posterior probabilities of churn for each observation in the test set. The point represent each predictions with the model we are evaluating, where blue points are the customers that decide to stay (`Churn`=No) and red points the ones that probably will abandond the company (`Churn`=Yes). The dashed horizontal line at $0.5$ serves as a decision threshold, indicating the probability level at which the model switches from predicting a customer will not churn to predicting they will churn. This allow us to give a visually representation of how the model is predicting.

## Bayes classifiers

### QDA

QDA refers to **Quadratic Discriminant Analysis**, this model assumes a Gaussian distributions of the predictors for each class but its covariance matrix can be different.

```{r qda_classifier}
# Train the QDA model using caret
model_QDA <- train(Churn ~ .,    
                    data = training_transf,
                    method = "qda",
                    trControl = train_ctrl) # Use the defined train control

# Predictions
predictions_QDA <- predict(model_QDA, X_test)
# head(predictions_QDA)

# Posterior probabilities
post_prob_QDA <- predict(model_QDA, X_test, type = "prob")
# head(post_prob_QDA)

# Performance measures
confMat_QDA <- table(predictions_QDA, testing_transf$Churn)
addmargins(confMat_QDA)
```


```{r qda_measures, echo=FALSE}
sprintf("------ Testing measures ------")
# Error of the model
error_QDA <- (n_test - sum(diag(confMat_QDA))) / n_test
sprintf("Error for QDA: %f", error_QDA)

# Test accuracy
acc_test_QDA <- sum(diag(confMat_QDA)) / n_test
sprintf("Accuracy for QDA: %f", acc_test_QDA)
```


```{r qda_plot, fig.align='center', out.width="70%"}
# Plot
ggplot(data = testing_transf, aes(x = seq_along(testing_transf$Churn))) +
  geom_point(aes(y = post_prob_QDA[,"Yes"], color = Churn)) +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("No" = "deepskyblue2", "Yes" = "firebrick2")) +
  xlab("Individual") +
  ylab("Probability of Churn with QDA") +
  geom_hline(yintercept = 0.5, linetype = "dashed")
```

The confusion matrix and the plot represent the same idea in different manner. From the plot, we observe that the red points below and blue points above the threshold line are point that has been misclassifications. Meanwhile in the confusion matrix this clients can be found over the no-diagonal values.

We can conclude that the model:

- It is good at identifying the customers who will stay and actually stay at the company ($90.26$% right).

- The worst mistake is when the model predict the client will stay and he actually left, and with QDA happens $86$ times.

- The other possible error is when it predict the client will leave, but its real choice is that he will continue with the company ($237$ wrong identifications).

- Finally, the model just gets right un $54.77$% when predicting the client will leave and actually leave.

- Furthermore, the kappa value is not too high.


### LDA

**Linear Discriminant Analysis** (LDA) is a simplified version of QDA model which introduce some bias to reduce the variance.

```{r lda_classifier}
# Train the LDA model using caret
model_LDA <- train(Churn ~ .,    
                    data = training_transf,
                    method = "lda",
                    trControl = train_ctrl) # Use the defined train control

# Predictions
predictions_LDA <- predict(model_LDA, X_test)
# head(predictions_LDA)

# Posterior probabilities
post_prob_LDA <- predict(model_LDA, X_test, type = "prob")
# head(post_prob_LDA)

# Performance measures
confMat_LDA <- table(predictions_LDA, testing_transf$Churn)
addmargins(confMat_LDA)
```


```{r lda_measures, echo=FALSE}
sprintf("------ Testing measures ------")
# Error of the model
error_LDA <- (n_test - sum(diag(confMat_LDA))) / n_test
sprintf("Error for QDA: %f", error_LDA)

# Test accuracy
acc_test_LDA <- sum(diag(confMat_LDA)) / n_test
sprintf("Accuracy for QDA: %f", acc_test_LDA)
```


```{r lda_plot, fig.align='center', out.width="70%"}
# Plot
ggplot(data = testing_transf, aes(x = seq_along(testing_transf$Churn))) +
  geom_point(aes(y = post_prob_LDA[,"Yes"], color = Churn)) +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("No" = "deepskyblue2", "Yes" = "firebrick2")) +
  xlab("Individual") +
  ylab("Probability of Churn with LDA") +
  geom_hline(yintercept = 0.5, linetype = "dashed")
```
This model achieved an accuracy of $81.3077$% improving the QDA performance, thus we can conclude it is able to make good predictions about if a client will churn or not.

However compared to QDA, the errors that suppose worse for the company have increase from $86$ to $163$. However, the mistakes from a client leaving the company but actually stay they really decrease. As before it accurate will identify people who will stay with the company.


### Naïve Bayes

Model useful when the dataset is very large and on multi-class predictors. For this dataset might not work as well as QDA or LDA. On the contrary, it usually works good for predictor that are independence one from each other.

```{r nb_classifier}
# Train Naive Bayes model
model_NB <- train(x = X_train, 
                  y = y_train, 
                  method = "naive_bayes", 
                  trControl = train_ctrl,
                  tuneLength = data.frame(laplace = 0.5,
                                      usekernel = FALSE,
                                      adjust = FALSE))


# Predict on test data
predictions_NB <- predict(model_NB, newdata = X_test)
# head(predictions_NB)

# Posterior probabilities
post_prob_NB <- predict(model_NB, X_test, type = "prob")
# head(post_prob_NB)

# Evaluate the model using confusion matrix
confusionMatrix(predictions_NB, y_test)
```


```{r nb_ignore, eval=FALSE, include=FALSE}
# Some manual calculation, however better use the confusionMatrix of caret

# Performance measures -> without using caret
confMat_NB <- table(predictions_NB, testing_transf$Churn)
addmargins(confMat_NB)

sprintf("------ Testing measures ------")
# Error of the model
error_NB <- (n_test - sum(diag(confMat_NB))) / n_test
sprintf("Error for QDA: %f", error_NB)

# Test accuracy
acc_test_NB <- sum(diag(confMat_NB)) / n_test
sprintf("Accuracy for QDA: %f", acc_test_NB)
```


```{r nb_measures, fig.align='center', out.width="70%"}
# Plot
ggplot(data = testing_transf, aes(x = seq_along(testing_transf$Churn))) +
  geom_point(aes(y = post_prob_NB[,"Yes"], color = Churn)) +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("No" = "deepskyblue2", "Yes" = "firebrick2")) +
  xlab("Individual") +
  ylab("Probability of Churn with Naïve Bayes") +
  geom_hline(yintercept = 0.5, linetype = "dashed")
```

In summary, the model achieved an accuracy of $78.96$%, which, while is not the highest accuracy among the models tested, is still pretty good. It demonstrates a string ability to correctly identify the client that will remain with the company (*Positive predictive Value*). However, regarding to the wrong predictions, it can be notice that it makes $108$ huge mistakes (which is in the average of the previous models).  


## Logistic regression

Logistic regression is a statistical method for predicting binary problems which allow also use categorical predictors. It looks like a model that can fit pretty well with my classification task. However, it must be notice that it is sensitive to outliers on the data.

```{r lr_classifier}
# Train Logistic Regression classifier
model_LR <- train(Churn ~.,
                  data = training_transf,
                  method = "glm",
                  family = "binomial", # For binary logistic regression
                  trControl = train_ctrl)

# Predict on test data
predictions_LR <- predict(model_LR, X_test)
# head(predictions_LR)

# Posterior probabilities
post_prob_LR <- predict(model_LR, X_test, type = "prob")
# head(post_prob_LR)

# Evaluate the model using confusion matrix
confusionMatrix(predictions_LR, y_test)
```


```{r lr_ignore, eval=FALSE, include=FALSE}
# Some manual calculation, however better use the confusionMatrix of caret

# Performance measures -> without using caret
confMat_LR <- table(predictions_LR, testing_transf$Churn)
addmargins(confMat_LR)

sprintf("------ Testing measures ------")
# Error of the model
error_LR <- (n_test - sum(diag(confMat_LR))) / n_test
sprintf("Error for QDA: %f", error_LR)

# Test accuracy
acc_test_LR <- sum(diag(confMat_LR)) / n_test
sprintf("Accuracy for QDA: %f", acc_test_LR)
```


```{r lr_measures, fig.align='center', out.width="70%"}
# Plot
ggplot(data = testing_transf, aes(x = seq_along(testing_transf$Churn))) +
  geom_point(aes(y = post_prob_LR[,"Yes"], color = Churn)) +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("No" = "deepskyblue2", "Yes" = "firebrick2")) +
  xlab("Individual") +
  ylab("Probability of Churn with Logistic regression") +
  geom_hline(yintercept = 0.5, linetype = "dashed")
```

To conclude, the model's accuracy is $81.6631$% so it can correctly predict both retainment and churn. The *sensitivity* is high, showing case the model is very good at identifying customers who will stay with the company services. However, this model tend to predict wrongly those clients which will leave (*specificity*). Finally, we can observed that the most important errors achieved to $168$ wrong points classification.


## Incorporing economic impact

From all models compute, I choose as the best one the one with lowest error and highest accuracy. This models is the Logistic Regression classifier. Also, if we take a look at the ROC curves, we can also conclude that it is better to use also the Logistic regression classifier.

```{r}
# Calculate ROC objects for each model
roc_LDA <- roc(response = testing_transf$Churn,
               predictor = post_prob_LDA[,"Yes"],
               levels = c("No", "Yes"))
roc_QDA <- roc(response = testing_transf$Churn,
               predictor = post_prob_QDA[,"Yes"],
               levels = c("No", "Yes"))
roc_NB <- roc(response = testing_transf$Churn, 
              predictor = post_prob_NB[,"Yes"],
              levels = c("No", "Yes"))
roc_LR <- roc(response = testing_transf$Churn,
              predictor = post_prob_LR[,"Yes"], 
              levels = c("No", "Yes"))

# Create a data frame for ggplot2
roc_data <- data.frame(
  Specificity = c(1 - roc_LDA$specificities,
                  1 - roc_QDA$specificities,
                  1 - roc_NB$specificities,
                  1 - roc_LR$specificities),
  Sensitivity = c(roc_LDA$sensitivities,
                  roc_QDA$sensitivities,
                  roc_NB$sensitivities, 
                  roc_LR$sensitivities),
  Model = factor(rep(c("LDA", "QDA", "Naive Bayes", "Logistic Regression"),
                     each=length(roc_LDA$sensitivities)))
)

# Plot ROC curves
ggplot(roc_data, 
       aes(x=Specificity, y=Sensitivity, color=Model)) +
  geom_line() +
  geom_abline(linetype="dashed", color="gray") +  # Add a diagonal dashed line
  scale_color_manual(values=c("darkorange",
                              "deepskyblue4", 
                              "chartreuse4",
                              "purple")) +
  labs(title="ROC Curves Comparison", x="1 - Specificity", y="Sensitivity") +
  theme_minimal() +
  theme(legend.title=element_blank(), legend.position="bottom") +
  annotate("text", x=0.75, y=0.25,
           label=paste("AUC LDA:", round(auc(roc_LDA), 3)), 
           color="darkorange") +
  annotate("text", x=0.75, y=0.20,
           label=paste("AUC QDA:", round(auc(roc_QDA), 3)), 
           color="deepskyblue4") +
  annotate("text", x=0.75, y=0.15, 
           label=paste("AUC NB:", round(auc(roc_NB), 3)),
           color="chartreuse4") +
  annotate("text", x=0.75, y=0.10, 
           label=paste("AUC LR:", round(auc(roc_LR), 3)), 
           color="purple")
```

```{r}
## Some pruebas --------------------------------------------------------------
library(plotly)
library(pROC)

fig <- plot_ly() %>%
  add_trace(x = 1 - roc_LDA$specificities, 
            y = roc_LDA$sensitivities, type = 'scatter', mode = 'lines',
            name = paste("LDA - AUC:", round(auc(roc_LDA), 3)),
            line = list(color = 'darkorange')) %>%
  add_trace(x = 1 - roc_QDA$specificities, y = roc_QDA$sensitivities, type = 'scatter', mode = 'lines',
            name = paste("QDA - AUC:", round(auc(roc_QDA), 3)),
            line = list(color = 'deepskyblue2')) %>%
  add_trace(x = 1 - roc_NB$specificities, y = roc_NB$sensitivities, type = 'scatter', mode = 'lines',
            name = paste("Naïve Bayes - AUC:", round(auc(roc_NB), 3)),
            line = list(color = 'chartreuse4')) %>%
  add_trace(x = 1 - roc_LR$specificities, y = roc_LR$sensitivities, type = 'scatter', mode = 'lines',
            name = paste("Logistic Regression - AUC:", round(auc(roc_LR), 3)),
            line = list(color = 'purple')) %>%
  layout(title = "ROC Curves Comparison",
         xaxis = list(title = "1 - Specificity", zeroline = FALSE),
         yaxis = list(title = "Sensitivity", zeroline = FALSE),
         legend = list(x = 0.1, y = 0.9))

fig # Print the plot
```



**Recall**. `Churn`: Yes = the customer left the company and No = the customer remained with the company.

On the previous models, we have assume all wrong prediction that makes the model have the same consequences for the company. However, this is not true. For instance, there are wrong prediction that can caused losses on the company. Imagine the model predict that a customer is going to still having the services (Churn=No) for the next period of time with this company, but it reality this client will leave (Churn=Yes) and canceld all the subscriptions with this company. It is clearly, that we will earn less money that we expected.
On the other hand, if the model predict that a client will leave but in reality he never though about leaving, thereby he will stay with this company. This will no suposse any problem or loss on the company's accounts.

If we take a look on the confusion matric of the logistic regression classifier:

| predictions_LR    |  No   | Yes   |
|-------------------|------:|------:|
| No                |  944  |  168  |
| Yes               |  90   |  205  |


Note: row (model's predictions) columns (actual/real value).

Worst error in the table is the one with committed 168 times with this model. Now, the goal is to decreasing this error by changing the probability threshold. And we can select the optimal threshold using some specific economic effects.

**Assumption used for doing this exercise**:
- If the company predicts a customer is going to stay and it actually stay, the company gains a 12% profil at the end of the quarter.
- If the company predicts a customer is going to stay but he leaves, then the loss is 100%.
- If the company predict a customer is going to leave but he actually stay, then the profit is 10%.
- If the company predict a customer is going to leave and he actually leave, then the profit is 0%.

Table of profits:

| Prediction/Actual |  Stay | Leave |
|-------------------|------:|------:|
| Stay              | 0.20  | -1.0  |
| Leave             | -0.01 | 0.03  |

For instance, a naive manager would incur a profit per applicant of $0.12\times0.93 - 1.0\times0.07 - 0.01\times0.0 + 0.0\times0.0 = 0.0416$

```{r}
# Confusion matrix values
TN = 944
FP = 168
FN = 90
TP = 205

# Profit/loss percentages
profit_TN = 0.20
loss_FP = -1.0
profit_FN = -0.01
profit_TP = 0.03

# Compute profit per applicant
profit_per_applicant = (TN * profit_TN + FP * loss_FP + FN * profit_FN + TP * profit_TP) / (TN + FP + FN + TP)
profit_per_applicant
```


This is the profit we should improve

Profit table as a vector:

```{r hyper-tuning_model_economic}
profit.unit <- c(0.20, -0.01, -1.0, 0.03)

## Selecting the optimal threshold to give the loan
profit.i = matrix(NA, nrow = 50, ncol = 10)
# 100 replicates for training/testing sets for each of the 10 values of threshold
 
p0=0.73
p1=1-p0

j <- 0

# This is for doing hyper-parameter
for (threshold in seq(0.05,0.5,0.05)){
  #for (p1 in c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9)){
  
  j <- j + 1
  cat(j)
  for(i in 1:50){
    
    # partition data intro training (40%) and testing sets (60%)
    d <- createDataPartition(training_transf$Churn, p = 0.4, list = FALSE)
    # select training sample
    
    train<-training_transf[d,]
    test <-training_transf[-d,]  
    
    #p1=1-p0
    # Fit logistic regression model
    full.model <- glm(Churn ~ ., data = training_transf, family = "binomial")
    
    # Obtain predicted probabilities on test set
    probabilities <- predict(full.model, testing_transf, type = "response")

    # Convert probabilities to predicted classes based on threshold
    Churn_pred <- ifelse(probabilities > threshold, "Yes", "No")
    
    # Create confusion matrix
    CM <- table(Churn_pred, testing_transf$Churn)

    # Calculate profit per applicant
    profit.applicant <- sum(profit.unit*CM)/sum(CM)
    profit.i[i,j] <- profit.applicant
    
  }
}

boxplot(profit.i, main = "Hyper-parameter selection",
        ylab = "unit profit",
        xlab = "threshold", names = seq(0.05,0.5,0.05),col="royalblue2")

# Storage all profits computed
all_profits_comp <- apply(profit.i, 2, median)
all_profits_comp

# Find the best threshold
best_threshold <- all_profits_comp[which.max(all_profits_comp)]
best_threshold
```

```{r model_economic}
## Final prediction for testing set using the optimal hyper-parameter:
# Fit the logistic regression model using the training data
final_model <- train(Churn ~ .,
                     data = training_transf, 
                     method = "glm", 
                     family = "binomial", 
                     trControl = train_ctrl)

# Predict probabilities on the test set
probabilities_post <- predict(model_LR, newdata = testing_transf, type = "prob")

# Make final predictions using the best threshold
final_predictions <- ifelse(probabilities_post[, "Yes"] > best_threshold, 
                            "Yes", "No")

# Confusion matrix
confusionMatrix(factor(final_predictions), testing_transf$Churn)
CM <- confusionMatrix(factor(final_predictions), testing_transf$Churn)$table


profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

As we can observed the profit of the company increase but we create a worse model. However, due to the importance of obtaining benefits, this model even though it make more mistakes it is much beneficial to the company.


\pagebreak
# Conclusion

```{r}
############ Summary ERRORS ############
## QDA -----------------------------------------------
# Sin balanceo, sin log-transformation: 23.52523%
# Sin balanceo, con log-transformation: 22.95665%
# Con balanceo, con log-transformation: 25.65743% (up)
## LDA -----------------------------------------------
# Sin balanceo, sin log-transformation: 18.8344%
# Sin balanceo, con log-transformation: 18.69225%
# Con balanceo, con log-transformation: 24.37811%
## NB ------------------------------------------------
# Sin balanceo, sin log-transformation: 22.10377%
# Sin balanceo, sin log-transformation: 21.39% (with caret)
# Sin balanceo, con log-transformation: 21.03767%

## Shrinkage -----------------------------------------

## LR ------------------------------------------------
# Sin balanceo, sin log-transformation: 19.62% (with caret)
# Sin balanceo, con log-transformation: 18.33689%
## Incorporing economic impact -----------------------
# Sin balanceo, con log-transformation: 58.85% (with caret)
```

\pagebreak
# References

```{r eval=FALSE, echo=FALSE}
# Returns the names of all packages loaded in the current R session
# knitr::write_bib(.packages(), "references.bib")
```

<div id="refs"></div>

\pagebreak
# Annex: fixing unbalanced dataset {#annex}

In my specif situation, the method that has more sense is to downsampling. So I do not need to "make up" values. Downsampling, consist on reduce the majority class.

There are other manners to solve this issue as a hybrid method named **smote**. This technique down-sample the majority class and synthesize new data points in the minority class. This procedure we will be implemented on the following section and it will be establishes as a common configuration for all statistical tools.


For all this models, we configure a common train control parameter.

```{r eval=FALSE}
# Now, it is time to solve the umblance of my dataset
# Using Synthetic Minority Oversampling Technique to produce more samples where customers churn out

# Set up train control -> 5 repeats of 10-fold cross validation
# Resolve class imbalances -> sampling = "down"
train_ctrl_balanced <- trainControl(method = "repeatedcv",
                                    repeats = 5,
                                    number = 10,
                                    sampling = "down")

```


## QDA

```{r eval=FALSE}
# Train the QDA model using caret
model_QDA_b <- train(Churn ~ .,    
                    data = training_transf,
                    method = "qda",
                    trControl = train_ctrl_balanced) # Use the defined train control
print(model_QDA_b)

# Predictions
predictions_QDA_b <- predict(model_QDA_b, X_test)
head(predictions_QDA_b)

# Posterior probabilities
post_prob_QDA_b <- predict(model_QDA_b, X_test, type = "prob")
head(post_prob_QDA_b)

# Performance measures
confMat_QDA_b <- table(predictions_QDA_b, testing_transf$Churn)
addmargins(confMat_QDA_b)

# Error of the model
error_QDA_b <- (n_test - sum(diag(confMat_QDA_b))) / n_test
error_QDA_b
```


## LDA

```{r eval=FALSE}
# Ensure the correct train control is used for the balanced LDA model
model_LDA_b <- train(Churn ~ .,    
                    data = training_transf,
                    method = "lda",
                    trControl = train_ctrl_balanced) # Correctly use the balanced train control
print(model_LDA_b)

# Predictions for the balanced LDA model
predictions_LDA_b <- predict(model_LDA_b, X_test)
head(predictions_LDA_b)

# Posterior probabilities for the balanced LDA model
post_prob_LDA_b <- predict(model_LDA_b, X_test, type = "prob")
head(post_prob_LDA_b)

# Performance measures for the balanced LDA model
confMat_LDA_b <- table(predictions_LDA_b, testing_transf$Churn)
addmargins(confMat_LDA_b)

# Calculate the error rate for the balanced LDA model
error_LDA_b <- (n_test - sum(diag(confMat_LDA_b))) / n_test
error_LDA_b
```


## Naïve Bayes

```{r eval=FALSE}
# Train Naive Bayes model
model_NB <- train(x = X_train, 
                  y = y_train, 
                  method = "naive_bayes", 
                  trControl = train_ctrl,
                  tuneLength = data.frame(laplace = 0.5,
                                      usekernel = FALSE,
                                      adjust = FALSE))


# Predict on test data
predictions_NB <- predict(model_NB, newdata = X_test)
head(predictions_NB)

# Posterior probabilities
post_prob_NB <- predict(model_NB, X_test, type = "prob")
head(post_prob_NB)

# Evaluate the model using confusion matrix
confusionMatrix(predictions_NB, y_test)

# Performance measures -> withou using caret
confMat_NB <- table(predictions_NB, testing_transf$Churn)
addmargins(confMat_NB)

# Error of the model
error_NB <- (n_test - sum(diag(confMat_NB))) / n_test
error_NB
```


## Logistic regression

```{r eval=FALSE}
model_LR <- train(Churn ~.,
                  data = training_transf,
                  method = "glm",
                  family = "binomial", # For binary logistic regression
                  trControl = train_ctrl)

# Print the model summary
print(model_LR)

# Predict on test data
predictions_LR <- predict(model_LR, X_test)
head(predictions_LR)

# Posterior probabilities
post_prob_LR <- predict(model_LR, X_test, type = "prob")
head(post_prob_LR)

# Evaluate the model using confusion matrix
confusionMatrix(predictions_LR, y_test)

# Performance measures -> without using caret
confMat_LR <- table(predictions_LR, testing_transf$Churn)
addmargins(confMat_LR)

# Error of the model
error_LR <- (n_test - sum(diag(confMat_LR))) / n_test
error_LR
```