---
title: 'Project 2: Machine-learning tools'
author: "Gabriela Levenfeld Sabau"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes   
    toc: TRUE              
    toc_float: TRUE
  geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
  mathjax: local
  self_contained: false
subtitle: MS in Statistics for Data Science
bibliography: references.bib
nocite: '@*'
link-citations: true
linkcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Dataset description.

Dataset obtained from **UC Irvine Machine Learning Repository** [@misc_chronic_kidney_disease_336].

## Variables description.
**Quantitative**
bp		-	Blood pressure
bgr		-	Blood glucose random
bu		-	Blood urea
sc		-	Serum creatinine
sod		-	Sodium
pot		-	Potassium
hemo	-	Hemoglobin
pcv		- Packed cell volume
wbcc	-	White blood cell count
rbcc	-	Red blood cell count

**Qualitative, Categorical**
All this $3$ variables has a order.
sg		-	Specific gravity
al		- Albumin
su		-	Sugar

**Qualitative, Binary**
rbc		-	Red blood cells
pc		-	Pus cell
pcc		-	Pus cell clumps
ba		-	Bacteria
htn		-	Hypertension
dm		-	Diabetes mellitus
cad		-	Coronary artery disease
appet	-	Appetite
pe		-	Pedal edema
ane		-	Anemia

**Target**
class	-	class

\pagebreak
# Data preprocessing

This section focuses on preparing the dataset for exploratory data analysis (EDA) and applying the computational models. This include loading the necessary material, handling missing values, encoding categorical variables, and splitting the dataset into training and testing subsets, among other tasks.

## Load material.

**Load libraries**. Several libraries are need for the project in order to work properly.

```{r load_libraries, message=FALSE, warning=FALSE}
library(skimr) # Summary statistics
library(tidyverse) # Include ggplot2, dplyr, among others
library(pander)
library(caret) # Models
library(mice) # Imputation missing data
library(VIM) # Imputation plots
library(GGally) # Plots of the continuous variables
library(psych) # Compute the skewness and correlation matrix
library(polycor) # Compute correlation heatmap mix data type
library(patchwork) # Plot grid
library(gridExtra) # For grid.arrange
library(corrplot) # Correlation matrix
library(reshape2) # Data reshaping (melt)
```

**Load dataset**. The dataset was store as an .arff file, in order to open the dataset I make some arrangement in python enviroment. I attached the code used in order to generate a .csv so I can work with during the rest of the project.

```{python open_file_python, eval=FALSE}
# Require libraries ------------------------------------------------------------
from scipy import io
import pandas as pd

# Load .arff file: extract the df and metadata ---------------------------------
dataframe, meta = io.arff.loadarff('data/chronic_kidney_disease_full.arff')
# Transform into pd df for handling
dataframe = pd.DataFrame(dataframe) # Some variables are into b'value' (bad format)

# Fixing bad format ------------------------------------------------------------
def bytes_to_int(byte_value):
    try:
        return byte_value.decode('utf-8') # Decode bytes to a string
    except AttributeError:
        return byte_value # When it is already a string
      
# Apply the conversion function to the specified columns in df
for column in dataframe.columns:
    dataframe[column] = dataframe[column].apply(bytes_to_int)

# Save the df to .csv file -----------------------------------------------------
dataframe.to_csv("data/chronic_kidney_disease_full.csv", index=False)
```

```{r load_dataset}
# Load new .csv dataset
data <- read.csv("data/chronic_kidney_disease_full.csv", header = TRUE, sep = ",")
```

**First look at the dataset**. Brief overview of the dataset for further preprocessing steps.

```{r small_EDA1, eval=FALSE, include=FALSE}
# Small Exploratory Data Analysis
skim(data)
# Presents results for every column; the statistics it provides depend on the class of the variable

# For visualizing the first 2 rows
head(data, 2)
```

```{r small_EDA2}
glimpse(data)
```

## Data cleaning and Feature engineering.

**Ensure consistency on the dataset**. Missing values are represented sometimes as "?" and other times as NA. To ensure consistency over the whole data, it is converted all missing values into the same format. 

```{r}
# Convert "?" to NA
data <- data.frame(lapply(data, function(x) ifelse(x == "?", NA, x)))
```


**Duplicate data**. There is no duplicated data.

```{r duplicate_data}
duplicates <- data[duplicated(data), ]
```

**Encoding qualitative variables**. The goal is to get a dataset that our statistical models knows how to manage. This include, turn it into factor the categorical variables. Meanwhile, binary encoding is easy, for the multi-state variables strategy is slightly different due to the presence of natural order. This means, that a higher lever of this variables are directly relevant for medical conditions, clearly affecting to medical conditions such as chronic kidney disease. This variables `sg`, `al` and `su` represent the specific gravity, albumin and sugar. In order to make the proper encoding, it is mandatory to specify the importance of the order when creating this levels using the factor function.

```{r data_cleaning, results='hide'}
# Binary variables -------------------------------------------------------------
# (Included the target variable)
bin_and_target_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane", "class")

for (var in bin_and_target_vars) {
  data[[var]] <- as.factor(data[[var]])
}

# Categorical variables --------------------------------------------------------
data$sg <- factor(data$sg, 
                  levels = c("1.005", "1.010", "1.015", "1.020", "1.025"), 
                  ordered = TRUE)
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4", "5"), 
                  ordered = TRUE)
data$su <- factor(data$su,
                  levels = c("0", "1", "2", "3", "4", "5"),
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
str(data) # To check all qualitative vars are in factor type
levels(data$sg) # Check the levels of a specific variable
table(data$al) # Check how many observation per level there is
```

**Drop categories on `al` and `su`**. `al` level $5$ contains just one observation, due to its low presence on the dataset it can be changed into level $4$ gaining stability and avoiding possible futures problems when imputing missing values. Same startegy is followed on `su` $4$ level. This approach assumes that the distinction between levels $4$ and $5$ is not significant.

```{r}
table(data$sg)
table(data$al)
table(data$su)
```

```{r drop_categories, results='hide'}
# Reduce categories on al and su -----------------------------------------------
# Transforming into numeric for easy replacement
data$al <- as.numeric(as.character(data$al))
data$su <- as.numeric(as.character(data$su))

# Change level: 5 -> 4
data$al[data$al == 5] <- 4
data$su[data$su == 5] <- 4

# Turn into factors (order) again without level 5
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4"),
                  ordered = TRUE)
data$su <- factor(data$su, 
                  levels = c("0", "1", "2", "3", "4"), 
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
table(data$al)
table(data$su)
```


## Identify missing data.

Next step focus on identify missing values on the dataset that can be due to typo errors, absence of this patient answers, etc.

This dataset contains missing values, we must handle them in order to use the models (not all models support NA's). The above table shows the missing values by columns express in percentage. None of this variables superan a threshold of $60$% of missing values thus it is recommend to not delete any variable avoiding loosing this information.

```{r counting_NAs}
total_na <- sum(is.na(data)) # 1012 NA

# Counting NA's ----------------------------------------------------------------
missing_values <- colSums(is.na(data))
sort_missing_values <- sort(missing_values, decreasing = TRUE)

percentage_na <- missing_values/nrow(data)*100 # NA's in %
sort(percentage_na, decreasing = TRUE)
```

**Delete patient information**. First approach is to take a look if there is a chance that data from a patient has not been well reported. There are $10$ observations that has $7$ columns with invalid data. These rows are deleted, there is no sense on make up around the $45.83$% of information of a patient.

```{r rm_rows}
# All observations with at least 1 NA ------------------------------------------
obvs_with_na <- data[rowSums(is.na(data)) > 0, ]
missing_counts <- rowSums(is.na(obvs_with_na))

# df for storing number of NAs and corresponding indexes
missing_info <- data.frame(
  count = names(table(missing_counts)),
  indexes = tapply(names(missing_counts), missing_counts, paste, collapse = ", ")
)

pander(missing_info)

# Delete patients with 11 NA variables -----------------------------------------
rows_to_delete <- c(60, 87, 105, 149, 166, 223, 229)
data <- data[!rownames(data) %in% rows_to_delete, ]
```

## Split the dataset.

This step consist on divide the dataset into two subsets. The training set, takes $80$% of the data and it is used to train the models. This subset allows the model to learn patterns and relationships within the data. On the other hand, testing set ($20$% of the data) is used by unseen data to evaluate the performance of the trained model.

```{r split_dataset, results='hide'}
set.seed(1234) # For reproducibility

# Split into training (80%) and testing (20%) set
index <- createDataPartition(data$class, p=0.8, list=FALSE)
training <- data[ index,]
testing <- data[-index,]

nrow(training) # Number of observation for training
nrow(testing) # Number of observation for testing
```

## Handling missing data.

```{r mice_imputation, warning=FALSE, message=FALSE, results='hide'}
# MICE Imputation --------------------------------------------------------------
meth <- rep("rf", length = ncol(training)) # Set all methods to RF model

# Generates 4 imputations
mice_model <- mice(training,
                   m = 4, maxit = 5, method = meth,
                   seed = 1234, print = FALSE
                   )

# Complete the train data with the first imputation
training_imp <- complete(mice_model, 1)

# Function to impute new observations based on the previous imputation model
source("https://raw.githubusercontent.com/prockenschaub/Misc/master/R/mice.reuse/mice.reuse.R")

# Apply the imputation model to the testing set and extract the first imputation
testing_imp <- mice.reuse(mice_model, testing, maxit = 1)[[1]]
```

**Perform diagnostics of how the imputation is working**.

```{r performance_mice1, fig.align='center', fig.height=3, fig.width=4}
# Plot 1: Represent the % of missing data
aggr(training_imp,
     col = c("navyblue", "yellow"),
     numbers = TRUE,
     sortVars = FALSE,
     labels = names(training_imp),
     cex.axis = .7,
     gap = 3,
     ylab = c("Missing data", "Observed data")
)
```


```{r performance_mice2, fig.align='center', fig.height=4, fig.width=5}
# Plot 2: Check it follows the distribution of the original variable
densityplot(mice_model)

# Plot 3: Visualize the imputed values for the 'age' variable across multiple imputations
# https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html
stripplot(mice_model, pch = 19, xlab = "Imputation number")
```

\pagebreak
# Exploratory Data Analysis (EDA)

The aim is to uncover patterns, detect outliers, and test assumptions through visual and quantitative methods.

## Numeric variables.

The `ggpair` function from the GGally package [@GGally_ref], allow us to create a scatter plot matrix for this variables. This matrix includes histograms for each variable along the diagonal, showcasing their distribution. The lower triangle displays scatter plots to explore the relationships between variable pairs, while the upper triangle shows the correlation coefficients, providing a quantitative measure of their relationships. This information allow us to identify the shape distribution, tendencies, outliers, among other things, thanks to different views in one grid.

```{r scatterPlot_contiuous, fig.align='center', message=FALSE, warning=FALSE}
# Select continuous variables (include target)
scatterPlot_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc", "class")]

# Create scatter plot matrix
ggpairs(scatterPlot_data,
        aes(fill="pink"),
        lower = list(continuous = "points", combo = "box_no_facet"),
        upper = list(continuous = "cor"),
        diag = list(continuous = "barDiag"),
        title = "Scatter plot matrix"
       )
```

**General insights**.

1. This analysis reveals a non-normal distribution for the numeric variables. To improve the data distribution for machine learning methods, such as *Neural networks*, the data will be transformed depending on its skewness. This below table shows the skewness measures for each variable and it has been obtained from `psych` [@psych_ref] package. Log-transformation are applied to variables with positive skewness (`bp`, `bgr`, `bu`, `sc`, `pot` and `wbcc`), while a square transformation is used for `sod` with negative skewness. Notice that the threshold for decision-making is set at skewness values above $1$ or below $-1$.

```{r skewness_analysis, warning=FALSE, message=FALSE, echo=FALSE}
numeric_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")]
skewness <- round(skew(numeric_data), 1)
result_df <- data.frame(variable = colnames(numeric_data), skewness = skewness)
pander(result_df)
```

2. Standardization of Data. The data will be standardized to benefit models that are sensitive to the scale of the data, such as *K-Nearest Neighbors* and *Neural Networks*. This step will not affect negatively to the other models like *Gradient Boosting*, which are less sensitive to scaling.

3. About correlation coefficients. It is worth mentioning that this dataset contains high correlation between some variables, this value is identify when coefficients are grater than $>0.7$ or lower than $< -0.7$). For instance, packed cell volume (`pcv`) and hemoglobin (`hemo`) have a strong positive correlation coefficient of $0.869$. From a biologically point of view makes sense, both parameter are related with the amount of oxygen going through human blood. Hemoglobin is the protein in red blood cells that carries oxygen, while the packed cell volume represents the volume percentage of red blood cells in blood. Moreover, red blood cells count (`rbcc`) is also highly correlated with `hemo` ($0.770$) and `pcv` ($0.766$) indicating that if one of this parameter increase the other one will increase too.

4. Outliers. It is notably the presence of some outliers over the variables. This observations (referring to the outliers) represent individuals with extreme values probably due to underlying medical conditions or measurement errors. Notice that this observations may influence directly the model performance and the conclusion derived from the data analysis.

**Relation with the output**. A deeper exploration of the most correlated variables with the target, `class`, let some important insights on identify chronic kidney disease and healthy patients. First pattern obtained is that people with chronic kidney disease (class = `ckd`) tend to have lower values of hemoglobin (`hemo`), packed cell volume (`pcv`), and red blood cell count (`rbcc`) as indicated by the red density distributions. In contrast, healthy patients (class = `notckd`) have higher values for these parameters, as shown by blue density distribution. Moreover, boxplots demonstrated same insight as before, with median values of `hemo`, `pcv`, and `rbcc` being lower in the chronic kidney disease group. Last distinction observed on the boxplots is about on how the outliers exist just on unhealthy people distribution. This situation might highlight the complexity for diagnosis and identifying the chronic kidney disease due to weird and unexpected behavior of this patients. 

```{r deeperExploration, warning=FALSE, message=FALSE, fig.align='center'}
deeperExploration_vars <- training_imp[, c("hemo", "pcv", "rbcc", "class")]

# Most correlate variables separate by target
ggpairs(deeperExploration_vars,
        aes(color = class, fill = class), # Apply color based on 'class'
        lower = list(continuous = wrap("points", alpha = 0.5)), # Transparency
        upper = list(continuous = "cor"),
        diag = list(continuous = wrap("densityDiag", alpha = 0.5))
) + 
  theme(legend.position = "bottom")
```

**Data transformation code**.

```{r numeric_transf}
# Numeric transformation
vars_log_transf <- c('bp', 'bgr', 'bu', 'sc', 'pot', 'wbcc') # Note: No zeros on any var

########################## TRAINING dataset ##########################
training_transf <- training_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
training_transf <- training_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
training_transf[,c("sod")] <- (training_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])

########################## TESTING dataset ##########################
testing_transf <- testing_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
testing_transf <- testing_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
testing_transf[,c("sod")] <- (testing_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])
```

## Cualitative variables.

**Binary variables**. Some tables are display in order to understand better the dataset.
First table indicates that most of the patients have normal levels of red blood cells (`rbc`) and pus cell (`pc`). In the second table, the majority of data is regarding people without pus cell clumps (`pcc`) and bacteria (`ba`). The third table focus on various conditions, including hypertension (`htn`), diabetes mellitus (`dm`), coronary artery disease (`cad`), pedal edema (`pe`), and anemia (`ane`). Hypertension and diabetes mellitus seems as the more common conditions among the patients. Meanwhile the other parameters are present in no more than $19$% of the people tested, thus these conditions might not be commontly within the population. Finally, most people of the dataset report having a good appetite.
In summary, the binary variables are not equally distributed on this dataset.

```{r eda_binary_tables, echo=FALSE, warning=FALSE, message=FALSE}
# Tables for distributions -----------------------------------------------------
# Create tables depending on the possible output
summary_abnormal <- lapply(c("rbc", "pc"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_notpresent <- lapply(c("pcc", "ba"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_noYes <- lapply(c("htn", "dm", "cad", "pe", "ane"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_poor <- lapply(c("appet"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)


# Print tables
pander(summary_abnormal, format = "html", escape = FALSE)
pander(summary_notpresent, format = "html", escape = FALSE)
pander(summary_noYes, format = "html", escape = FALSE)
pander(summary_poor, format = "html", escape = FALSE)
```

The following grid of bar charts aim to show the target distribution across each binary variable. When looking at parameter related with infections, such as `ba` (bacteria) and `pcc` (pus cell clumps), these are less frequent in the overall dataset, but when they do occur, they are predominantly found in patients with kidney disease (class = `ckd`). In addition, poor appetite seems to be a common symptom in patients with ckd. Hypertension (`htn`) and diabetes mellitus (`dm`) are variables where storage a high number of sick patients with this conditions.

```{r eda_binary_plot, echo=FALSE, fig.align='center'}
binary_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane")

# Bar plots by output ----------------------------------------------------------
# Reshape the df to long
long_data <- pivot_longer(training_transf, 
                          cols = binary_vars, 
                          names_to = "variable", 
                          values_to = "value")

# Combined plots with facet_wrap
p <- ggplot(long_data, aes(x = value, fill = as.factor(class))) + 
  geom_bar(position = "dodge", stat = "count") +
  facet_wrap(~variable, scales = "free_x") + 
  labs(y = 'Count', fill = 'Class') + 
  theme_minimal() + 
  theme(axis.text.x = element_text(hjust = 1),
        strip.text.x = element_text(face = "bold")) + 
  scale_fill_brewer(palette = "Set2")

print(p) # Display grid
```

**Category variables**.

Next step consist on explore the $3$ multi-state variables: specific gravity (`sg`), albumin (`al`), and sugar (`su`). Specific gravity clearly shows that levels $1.020$ and $1.025$ are the common levels for healthy people. Conversely, lower states are more prevalent among the presence of chronic kidney disease. On the other hand, healthy population is expect to have a level $0$ of albumin as well as a low level of sugar. Otherwise, levels indicates a higly link to this chronic disease.

```{r eda_category_distribution, echo=FALSE, fig.align='center'}
# Plot for Specific Gravity (sg)
plot_sg <- ggplot(training_transf, aes(x = as.factor(sg), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Specific Gravity by target",
       x = "Specific Gravity",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))

# Plot for Albumin (al)
plot_al <- ggplot(training_transf, aes(x = as.factor(al), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Albumin by target",
       x = "Albumin",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))  

# Plot for Sugar (su)
plot_su <- ggplot(training_transf, aes(x = as.factor(su), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Sugar by target",
       x = "Sugar",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))

# Arrange plots in a grid
grid.arrange(plot_sg, plot_al, plot_su, ncol = 2)
```

Additionally, in order to confirm the above insights some stacked bar chart has been code. This plots shows the proportion of people with and without chronic kindey disease across the different categories of these three multi-state variables. All patterns are confirmed.

```{r eda_category_proportion, echo=FALSE, warning=FALSE, message=FALSE}
category_data <- melt(training_transf,
                      id.vars = "class",
                      measure.vars = c("sg", "al", "su"))

# Create the combined plot using the melted data
ggplot(category_data, aes(x = value, fill = class)) + 
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("notckd" = "darkolivegreen3", "ckd" = "brown3")) +
  labs(title = "Relationship with class",
       x = "", y = "Proportion") +
  facet_wrap(~variable, scales = "free_x", nrow = 2) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.background = element_blank(),
        strip.text.x = element_text(size = 10))
```

## Correlation matrix.

The heatmap represents the strength and direction of the relationships between the variables in the dataset, where the color intensity indicates the magnitude of the correlation. Dark colors, that can be either blue (positive) or red (negative), suggest a stronger relationship between variables. A positive coefficient means that an increase (or decrease) in one variable is associated with an increase (or decrease) in the other. Conversely, a negative correlation indicated that as one variables increase, the other decrease. And, color in the middle are interpreted as weak correlations.

For dealing with mixed data type, the `hetcor`function from the `psych` [@psych_ref] package in R is used. This enable the computation of correlation across numeric, binary and multi-state variables as long as the qualitative variable are represented in factors.

```{r corr_heatmap}
# This apporach not working for two of the variables
training_no_cat <- training_transf[, !(names(training_transf) %in% c("al", "htn"))]

# Works for mix variables: binay, multi-state and numeric
# Qualitative variables have to be in factor
cor_matrix <- hetcor(training_no_cat)
cor_values <- cor_matrix$correlations
corrplot(cor_values, method="color", type="upper", order="hclust",
         tl.cex=0.7, tl.col="black", tl.srt=45)
```

**Analytical exploration**

```{r}
cor_flat <- as.data.frame(as.table(cor_values))
cor_flat <- cor_flat[order(-abs(cor_flat$Freq)),]

# Filter correlations: >0.7 or <-0.7
strong_correlations <- subset(cor_flat, abs(Freq) > 0.7 & Var1 != Var2)

# Remove duplicates rows
strong_correlations$pairID <- apply(strong_correlations[, c("Var1", "Var2")], 
                                    1,
                                    function(x) paste(sort(x), collapse = "-"))
strong_correlations <- strong_correlations[!duplicated(strong_correlations$pairID),]
strong_correlations$pairID <- NULL # Remove ID

# Add sign
strong_correlations$Sign <- ifelse(strong_correlations$Freq > 0, "+", "-")

pander(strong_correlations)
head(strong_correlations, 5)
```



## Target variable.

To see the distribution of our target variable (`class`), we first look at the raw counts and then at the percentages to understand the proportion of sickness patients. To conclude, it is worth mentioning that the output varibale is not imbalanced, thereby there is no need for upsampling or downsampling this parameter.

```{r target_distribution}
# Check counts of chronic kidney disease
pander(table(data$class))

# Results in percentage
pander(prop.table(table(data$class)) * 100)
```

\pagebreak
# Machine-learning tools

First, it is defined some common consideration that I will assume for all models implemented.

1º. The dataset is divided into predictors and the target variable.

```{r split_subsets}
# Divide into predictors and target both subsets
X_train <- training_transf %>% dplyr::select(-class)
y_train <- training_transf$class

X_test <- testing_transf %>% dplyr::select(-class)
y_test <- testing_transf$class
n_test <- dim(testing_transf)[1]
```

2º. **Economic assumption used**. Incorporating a custom metric in Caret.

- If the doctor correctly diagnose a patience does not have a chronic kidney disease, the 

**Economic assumption used**.

- If the company predicts a customer is going to stay and it actually stay, the company gains a 12% profit at the end of the quarter.

- If the company predicts a customer is going to stay but he leaves, the loss is 100%.

- If the company predict a customer is going to leave but he actually stay, the profit is 10%.

- If the company predict a customer is going to leave and he actually leave, then the profit is 3%.

Using the specified profits and losses for each prediction outcome, we compute the average profit per customer based on the model's predictions.

```{r eval=FALSE}
# lev=NULL is mandatory to get the k. So caret can interprretate this is the accuracy (or somthing like that ha dicho)
profit_unit <- c(0.12, -0.01, -1.0, 0.0)
EconomicProfit <- function(data, lev = NULL, model = NULL) 
{
  y_pred = data$pred 
  y_true = data$obs
  CM = confusionMatrix(y_pred, y_true)$table
  out = sum(profit_unit*CM)/sum(CM)
  names(out) <- c("EconomicProfit")
  out
}
```

3º. Control to optimize the hyper-parameters.

```{r eval=FALSE}
ctrl <- trainControl(method = "repetcv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = EconomicProfit,
                     verboseIter=T)
```

## k-Nearest Neighbors

```{r eval=FALSE}
knnFit <- train(class ~ ., 
                method = "knn", 
                data = training_transf,
                tuneLength = 7,
                metric = "EconomicProfit",
                trControl = ctrl)
# We use 7 values of k
print(knnFit)
```


## SVM
## Decision trees, Random Forests, Gradient Boosting
X-gradiend boosting
## Neural Networks
Y tb Deep.NN

\pagebreak
# References

<div id="refs"></div>
