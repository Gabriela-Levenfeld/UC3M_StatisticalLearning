---
title: 'Project 2: Machine-learning tools'
author: "Gabriela Levenfeld Sabau"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes   
    toc: TRUE              
    toc_float: TRUE
  geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
  mathjax: local
  self_contained: false
subtitle: MS in Statistics for Data Science
#bibliography: references.bib
#nocite: '@*'
link-citations: true
linkcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Dataset description.

Dataset obtained from **UC Irvine Machine Learning Repository** [@misc_chronic_kidney_disease_336].

## Variables description.
**Quantitative**
bp		-	Blood pressure
bgr		-	Blood glucose random
bu		-	Blood urea
sc		-	Serum creatinine
sod		-	Sodium
pot		-	Potassium
hemo	-	Hemoglobin
pcv		- Packed cell volume
wbcc	-	White blood cell count
rbcc	-	Red blood cell count

**Qualitative, Categorical**
All this $3$ variables has a order.
sg		-	Specific gravity
al		- Albumin
su		-	Sugar

**Qualitative, Binary**
rbc		-	Red blood cells
pc		-	Pus cell
pcc		-	Pus cell clumps
ba		-	Bacteria
htn		-	Hypertension
dm		-	Diabetes mellitus
cad		-	Coronary artery disease
appet	-	Appetite
pe		-	Pedal edema
ane		-	Anemia

**Target**
class	-	class

\pagebreak
# Data preprocessing

This section focuses on preparing the dataset for exploratory data analysis (EDA) and applying the computational models. This include loading the necessary material, handling missing values, encoding categorical variables, and splitting the dataset into training and testing subsets, among other tasks.

## Load material.

**Load libraries**. Several libraries are need for the project in order to work properly.

```{r load_libraries, message=FALSE, warning=FALSE}
library(skimr) # Summary statistics
library(tidyverse) # Include ggplot2, dplyr, among others
library(pander)
library(caret) # Models
library(mice) # Imputation missing data
library(VIM) # Imputation plots
library(GGally) # Plots of the continuous variables
library(psych) # Compute the skewness and correlation matrix
library(polycor) # Compute correlation heatmap mix data type
```

**Load dataset**. The dataset was store as an .arff file, in order to open the dataset I make some arrangement in python enviroment. I attached the code used in order to generate a .csv so I can work with during the rest of the project.

```{python open_file_python, eval=FALSE}
# Require libraries ------------------------------------------------------------
from scipy import io
import pandas as pd

# Load .arff file: extract the df and metadata ---------------------------------
dataframe, meta = io.arff.loadarff('data/chronic_kidney_disease_full.arff')
# Transform into pd df for handling
dataframe = pd.DataFrame(dataframe) # Some variables are into b'value' (bad format)

# Fixing bad format ------------------------------------------------------------
def bytes_to_int(byte_value):
    try:
        return byte_value.decode('utf-8') # Decode bytes to a string
    except AttributeError:
        return byte_value # When it is already a string
      
# Apply the conversion function to the specified columns in df
for column in dataframe.columns:
    dataframe[column] = dataframe[column].apply(bytes_to_int)

# Save the df to .csv file -----------------------------------------------------
dataframe.to_csv("data/chronic_kidney_disease_full.csv", index=False)
```

```{r load_dataset}
# Load new .csv dataset
data <- read.csv("data/chronic_kidney_disease_full.csv", header = TRUE, sep = ",")
```

**First look at the dataset**. Brief overview of the dataset for further preprocessing steps.

```{r small_EDA1, eval=FALSE, include=FALSE}
# Small Exploratory Data Analysis
skim(data)
# Presents results for every column; the statistics it provides depend on the class of the variable

# For visualizing the first 2 rows
head(data, 2)
```

```{r small_EDA2}
glimpse(data)
```

## Data cleaning and Feature engineering.

**Ensure consistency on the dataset**. Missing values are represented sometimes as "?" and other times as NA. To ensure consistency over the whole data, this can be done by converting all missing values if exist into same format. 

```{r}
# Convert "?" to NA
data <- data.frame(lapply(data, function(x) ifelse(x == "?", NA, x)))
```


**Duplicate data**. There is no duplicated data.

```{r duplicate_data}
duplicates <- data[duplicated(data), ]
```

**Encoding qualitative variables**. The goal is to get a dataset that our statistical models knows how to manage. This include, turn it into factor the categorical variables. Meanwhile, binary encoding is easy, for the multi-state variables strategy is slightly different due to the presence of natural order. This means, that a higher lever of this variables are directly relevant for medical conditions, clearly affecting to medical conditions such as chronic kidney disease. This variables `sg`, `al` and `su` represent the specific gravity, albumin and sugar. In order to make the proper encoding, it is mandatory to specify the importance of the order when creating this levels using the factor function.

```{r data_cleaning, results='hide'}
# Binary variables -------------------------------------------------------------
# (Include the target variable)
bin_and_target_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane", "class")

for (var in bin_and_target_vars) {
  data[[var]] <- as.factor(data[[var]])
}

# Categorical variables --------------------------------------------------------
data$sg <- factor(data$sg, 
                  levels = c("1.005", "1.010", "1.015", "1.020", "1.025"), 
                  ordered = TRUE)
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4", "5"), 
                  ordered = TRUE)
data$su <- factor(data$su,
                  levels = c("0", "1", "2", "3", "4", "5"),
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
str(data) # To check all qualitative vars are in factor type
levels(data$sg) # Check the levels of a specific variable
table(data$al) # Check how many observation per level there is
```

**Drop categories on `al` and `su`**. `al` level $5$ contains just one observation, due to its low presence on the dataset it can be changed into level $4$ gaining stability and avoiding possible futures problems when imputing missing values. Same startegy is followed on `su` $4$ level. This approach assumes that the distinction between levels $4$ and $5$ is not significant.

```{r}
table(data$sg)
table(data$al)
table(data$su)
```

```{r drop_categories, results='hide'}
# Reduce categories on al and su -----------------------------------------------
# Transforming into numeric for easy replacement
data$al <- as.numeric(as.character(data$al))
data$su <- as.numeric(as.character(data$su))

# Change level: 5 -> 4
data$al[data$al == 5] <- 4
data$su[data$su == 5] <- 4

# Turn into factors (order) again without level 5
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4"),
                  ordered = TRUE)
data$su <- factor(data$su, 
                  levels = c("0", "1", "2", "3", "4"), 
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
table(data$al)
table(data$su)
```


## Identify missing data.

Next step focus on identify missing values on the dataset that can be due to typo errors, absence of this patient answers, etc.

This dataset contains missing values, we must handle them in order to use the models (not all models support NA's). The above table shows the missing values by columns express in percentage. None of this variables superan a threshold of $60$% of missing values thus it is recommend to not delete any variable avoiding loosing this information.

```{r counting_NAs}
total_na <- sum(is.na(data)) # 1012 NA

# Counting NA's ----------------------------------------------------------------
missing_values <- colSums(is.na(data))
sort_missing_values <- sort(missing_values, decreasing = TRUE)

percentage_na <- missing_values/nrow(data)*100 # NA's in %
sort(percentage_na, decreasing = TRUE)
```

**Delete patient information**. First approach is to take a look if there is a chance that data from a patient has not been well reported. There are $10$ observations that has between $7$ columns with invalid data. These rows are deleted, there is no sense on make up around the $45.83$% of information of a patient.

```{r}
# All observations with at least 1 NA ------------------------------------------
obvs_with_na <- data[rowSums(is.na(data)) > 0, ]
missing_counts <- rowSums(is.na(obvs_with_na))

# df for storing number of NAs and corresponding indexes
missing_info <- data.frame(
  count = names(table(missing_counts)),
  indexes = tapply(names(missing_counts), missing_counts, paste, collapse = ", ")
)

pander(missing_info)

# Delete patients with 11 NA variables -----------------------------------------
rows_to_delete <- c(60, 87, 105, 149, 166, 223, 229)
data <- data[!rownames(data) %in% rows_to_delete, ]
```

## Split the dataset.

This step consist on divide the dataset into two subsets. The training set, takes $80$% of the data and it is used to train the models. This subset allows the model to learn patterns and relationships within the data. On the other hand, testing set ($20$% of the data) is used by unseen data to evaluate the performance of the trained model.

```{r split_dataset, results='hide'}
set.seed(1234) # For reproducibility

# Split into training (80%) and testing (20%) set
index <- createDataPartition(data$class, p=0.8, list=FALSE)
training <- data[ index,]
testing <- data[-index,]

nrow(training) # Number of observation for training
nrow(testing) # Number of observation for testing
```

## Handling missing data.

```{r warning=FALSE, message=FALSE, results='hide'}
# MICE Imputation --------------------------------------------------------------
meth <- rep("rf", length = ncol(training)) # Set all methods to RF model

# Generates 4 imputations
mice_model <- mice(training,
                   m = 4, maxit = 5, method = meth,
                   seed = 1234, print = FALSE
                   )

# Complete the train data with the first imputation
training_imp <- complete(mice_model, 1)

# Function to impute new observations based on the previous imputation model
source("https://raw.githubusercontent.com/prockenschaub/Misc/master/R/mice.reuse/mice.reuse.R")

# Apply the imputation model to the testing set and extract the first imputation
testing_imp <- mice.reuse(mice_model, testing, maxit = 1)[[1]]
```

**Perform diagnostics of how the imputation is working**.

```{r performance_mice1, fig.align='center', fig.height=3, fig.width=4}
# Plot 1: Represent the % of missing data
aggr(training_imp,
     col = c("navyblue", "yellow"),
     numbers = TRUE,
     sortVars = FALSE,
     labels = names(training_imp),
     cex.axis = .7,
     gap = 3,
     ylab = c("Missing data", "Observed data")
)
```


```{r performance_mice2, fig.align='center', fig.height=4, fig.width=5}
# Plot 2: Check it follows the distribution of the original variable
densityplot(mice_model)

# Plot 3: Visualize the imputed values for the 'age' variable across multiple imputations
# https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html
stripplot(mice_model, pch = 19, xlab = "Imputation number")
```

\pagebreak
# Exploratory Data Analysis (EDA)

The aim is to uncover patterns, detect outliers, and test assumptions through visual and quantitative methods.

## Numeric variables.

`ggpair` function from the GGally package [@GGally_ref], allow us to create a scatter plot matrix for this variables. This matrix includes histograms for each variable along the diagonal, showcasing their distribution. The lower triangle displays scatter plots to explore the relationships between variable pairs, while the upper triangle shows the correlation coefficients, providing a quantitative measure of their relationships. This information allow us to identify the shape distribution, tendencies, outliers, among other things, thanks to different views in one grid.

```{r scatterPlot_contiuous, warning=FALSE, message=FALSE}
# Select continuous variables (include target)
scatterPlot_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc", "class")]

# Create scatter plot matrix
ggpairs(scatterPlot_data,
        aes(fill="pink"),
        lower = list(continuous = "points", combo = "box_no_facet"),
        upper = list(continuous = "cor"),
        diag = list(continuous = "barDiag"),
        title = "Scatter plot matrix"
       )
```

**General insights**.

- This analysis reveals non-normal distribution for the numeric variables, due to the used of some machine learning methods (as *Neural networks*), the data will be transformed depending on its skewness distribution. Posi


- Furtheremore data are going to be standardized. Models such as *Nearest Neighbors*, *Neural networks*, among others, will benefits and it will not affect to the other methods as *Gradient Boosting*. For more precision on the making-decision, `psych` package has been used. Conclusion reach thanks to the above table are apply log-transformation for positive skewness (`bp`, `bgr`, `bu`, `sc`, `pot` and `wbcc`) and ^2 for negative skewness (`sod`). Take into account that I consider presence of skewness for values above $1$ or below $-1$.

```{r warning=FALSE, message=FALSE,echo=FALSE}
numeric_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")]
skewness <- round(skew(numeric_data), 1)
result_df <- data.frame(variable = colnames(numeric_data), skewness = skewness)
pander(result_df)
```

- In addition, it is worth mentioning the high correlation ($>0.7$ or $< -0.7$) that exist between:
`pcv`-`hemo`: correlation coefficient of 0.869
`rbcc`-`hemo`: correlation coefficient of 0.770
`rbcc`-`pcv`: correlation coefficient of 0.766

- Finally, it is also notably that this parameters presents some outliers.

**Relation with the output**. We make a deeper exploration with the most correlate variables.

```{r deeperExploration, warning=FALSE, message=FALSE}
deeperExploration_vars <- training_imp[, c("hemo", "pcv", "rbcc", "class")]

ggpairs(deeperExploration_vars,
        aes(fill = class),
        lower = list(continuous = "points", combo = "box_no_facet"),
        upper = list(continuous = "cor"),
        diag = list(continuous = "densityDiag"),
        title = "Scatter plot matrix"
       ) +
  # FIXME: Plot overlapping
  ggplot2::geom_point(data = training_imp,
                      aes(x = hemo, y = pcv, color = class), alpha = 0.5) +
  ggplot2::geom_point(data = training_imp,
                      aes(x = hemo, y = rbcc, color = class), alpha = 0.5) +
  ggplot2::geom_point(data = training_imp,
                      aes(x = pcv, y = rbcc, color = class), alpha = 0.5)

```

**Data transformation code**.

```{r numeric_transf}
# Numeric transformation
vars_log_transf <- c('bp', 'bgr', 'bu', 'sc', 'pot', 'wbcc') # Note: No zeros on any var

########################## TRAINING dataset ##########################
training_transf <- training_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
training_transf <- training_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
training_transf[,c("sod")] <- (training_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])

########################## TESTING dataset ##########################
testing_transf <- testing_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
testing_transf <- testing_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
testing_transf[,c("sod")] <- (testing_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])
```

## Cualitative variables.

**Binary variables**.



**Category variables**.

## Correlation matrix

The heatmap represents the strength and direction of the relationships between the variables in the dataset, where the color intensity indicates the magnitude of the correlation. Dark colors, that can be either blue (positive) or red (negative), suggest a stronger relationship between variables. A positive coefficient means that an increase (or decrease) in one variable is associated with an increase (or decrease) in the other. Conversely, a negative correlation indicated that as one variables increase, the other decrease. And, color in the middle are interpreted as weak correlations.

For dealing with mixed data type, the `hetcor`function from the `psych` [@psych_ref] package in R is used. This enable the computation of correlation across numeric, binary and multi-state variables as long as the qualitative variable are represented in factors.

```{r correlation_heatmap}
# Works for mix variables: binay, multi-state and numeric
# Qualitative variables have to be in factor

# FIXME: Not working due to al, htn and class errors
cor_matrix <- hetcor(training_transf)
cor_values <- cor_matrix$correlations
corrplot(cor_values, method="color", type="upper", order="hclust",
         tl.cex=0.7, tl.col="black", tl.srt=45)
```

**Analytical exploration**


## Target variable

To see the distribution of our target variable (`class`), we first look at the raw counts and then at the percentages to understand the proportion of sickness patients. To conclude, it is worth to mention there is no unbalance data on the target variable.

```{r target_distribution}
# Check counts of chronic kidney disease
pander(table(data$class))

# Results in percentage
pander(prop.table(table(data$class)) * 100)
```

\pagebreak
# Machine Learning

## k-Nearest Neighbors
## SVM
## Decision trees, Random Forests, Gradient Boosting
## Neural Networks
