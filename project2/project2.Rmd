---
title: 'Project 2: Machine-learning tools'
author: "Gabriela Levenfeld Sabau"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes   
    toc: TRUE              
    toc_float: TRUE
  geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
  mathjax: local
  self_contained: false
subtitle: MS in Statistics for Data Science
bibliography: references.bib
nocite: '@*'
link-citations: true
linkcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Dataset description.

The dataset used for developed this project is concerning chronic kidney disease and it is available on *UC Irvine Machine Learning Repository* [@misc_chronic_kidney_disease_336]. It contains medical data and laboratory parameters from $400$ patients, distributed across $24$ variables including blood pressure, potassium or appetite, among others.

## Variables description.

The information contained in the dataset can be classify based on its data type:

**Quantitative**.

- `age`; Age of the patient in years

- `bp`; Blood pressure (in mm/Hg).

- `bgr`; Blood glucose random (in mgs/dl).

- `bu`;	Blood urea (in mgs/dl).

- `sc`; Serum creatinine (in mgs/dl).

- `sod`; Sodium (mEq/L).

- `pot`; Potassium (mEq/L).

- `hemo`; Hemoglobin (gms).

- `pcv`; Packed cell volume.

- `wbcc`; White blood cell count (cells/cumm).

- `rbcc`; Red blood cell count (millions/cmm).

**Qualitative, Categorical**. *Variables with order.*

- `sg`; Specific gravity (possible levels: 1.005,1.010,1.015,1.020,1.025)

- `al`; Level of albumin (possible levels: 0,1,2,3,4,5).

- `su`; Level of sugar (possible levels: 0,1,2,3,4,5).

**Qualitative, Binary**.

- `rbc`; Whether the red blood cells is normal or abnormal.

- `pc`; Whether the pus cell is normal or abnormal.

- `pcc`; Presence of pus cell clumps.

- `ba`; Whether there the patient has a bacteria or not.

- `htn`; Presence of hypertension.

- `dm`; Presence of diabetes mellitus.

- `cad`; Presence of coronary artery disease.

- `appet`; Whether the appetite of the patient is good or poor.

- `pe`; Presence of pedal edema.

- `ane`; Presence of anemia.

**Target**.

- `class`; Indicates the presence (ckd) or absence (notckd) of chronic kidney disease.

## Goal.

By analyzing the dataset, the idea is to identify the key factors that allow to detect the presence of chronic kidney disease. This prediction task is a binary classification problem where the goal is to forecast one of the two possible outcomes: ckd (presence of the disease) or notckd (absence of the disease).

Such insights enable to develop strategies to prevent the progression of the disease and improve the quality of life for the affected patients.

\pagebreak
# Data preprocessing

This section focuses on preparing the dataset for exploratory data analysis (EDA) and applying the computational models. This include loading the necessary material, handling missing values, encoding categorical variables, and splitting the dataset into training and testing subsets, among other tasks.

## Load material.

**Load libraries**. Several R libraries are needed for the project in order to work properly.

```{r load_libraries, message=FALSE, warning=FALSE}
library(skimr) # Summary statistics
library(tidyverse) # Include ggplot2, dplyr, among others
library(pander)
library(caret) # Models
library(mice) # Imputation missing data
library(VIM) # Plots for imputation perfoormance
library(GGally) # Plots of the continuous variables
library(psych) # Compute the skewness and correlation matrix
library(polycor) # Compute correlation heatmap mix data type
library(gridExtra) # For grid.arrange
library(corrplot) # Correlation matrix
library(reshape2) # Data reshaping (melt)
library(pROC)
library(rattle) # Display DT diagrams
```

**Load dataset**. The dataset was stored in an .arff format. To open the file, I made some arrangements using Python enviroment, I attached the code used in order to generate the .csv so I can work with during the rest of the project.

```{python open_file_python, eval=FALSE}
# Require libraries ------------------------------------------------------------
from scipy import io
import pandas as pd

# Load .arff file: extract the df and metadata ---------------------------------
dataframe, meta = io.arff.loadarff('data/chronic_kidney_disease_full.arff')
# Transform into pd df for handling
dataframe = pd.DataFrame(dataframe) # Some variables are into b'value' (bad format)

# Fixing bad format ------------------------------------------------------------
def bytes_to_int(byte_value):
    try:
        return byte_value.decode('utf-8') # Decode bytes to a string
    except AttributeError:
        return byte_value # When it is already a string
      
# Apply the conversion function to the specified columns in df
for column in dataframe.columns:
    dataframe[column] = dataframe[column].apply(bytes_to_int)

# Save the df to .csv file -----------------------------------------------------
dataframe.to_csv("data/chronic_kidney_disease_full.csv", index=False)
```

```{r load_dataset}
# Load new .csv dataset
data <- read.csv("data/chronic_kidney_disease_full.csv", header = TRUE, sep = ",")
```

**First look at the dataset**. Brief overview of the dataset for further preprocessing steps.

```{r small_EDA1, eval=FALSE, include=FALSE}
# Small Exploratory Data Analysis
skim(data)
# Presents results for every column; the statistics it provides depend on the class of the variable

# For visualizing the first 2 rows
head(data, 2)
```

```{r small_EDA2}
glimpse(data)
```

## Data cleaning and Feature engineering.

**Ensure dataset consistency**.

The dataset contains missing values represented as "?" as well as "NA". To ensure consistency over the data, all missing values are converted to the same format, making easier future handling.

```{r consistency_dataset}
# Convert "?" to NA
data <- data.frame(lapply(data, function(x) ifelse(x == "?", NA, x)))
```


**Duplicate data**.

There is no duplicated data, each row represent a different patient.

```{r duplicate_data}
duplicates <- data[duplicated(data), ]
```

**Encoding qualitative variables**.

The goal is to get a dataset that our machine learning models know how to manage. This include, turn it into factor the categorical variables. Meanwhile, binary encoding is easy, for the multi-state variables strategy is slightly different due to the presence of natural order. This variables `sg` (specific gravity), `al` (albumin) and `su` (sugar), represent different ordered levels of severity or concentration that have impact on medical conditions, such as chronic kidney disease. In order to make the proper encoding, it is mandatory to specify the importance of the order when creating this levels using the factor function.

```{r data_cleaning, results='hide'}
# Binary variables -------------------------------------------------------------
# (Included the target variable)
bin_and_target_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane", "class")

for (var in bin_and_target_vars) {
  data[[var]] <- as.factor(data[[var]])
}

# Categorical variables --------------------------------------------------------
data$sg <- factor(data$sg, 
                  levels = c("1.005", "1.010", "1.015", "1.020", "1.025"), 
                  ordered = TRUE)
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4", "5"), 
                  ordered = TRUE)
data$su <- factor(data$su,
                  levels = c("0", "1", "2", "3", "4", "5"),
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
str(data) # To check all qualitative vars are in factor type
levels(data$sg) # Check the levels of a specific variable
table(data$al) # Check how many observation per level there are
```

**Drop categories**.

When analysing `al` and `su` parameters, it can be observed that level $5$ in both variables contains just one observations. This issue can introduced instability as well as future problem in missing value imputation. To address this, observations in these levels are merged into level $4$, under the assumption that the distinction between these levels is not really significantly.

```{r}
table(data$sg)
table(data$al)
table(data$su)
```

```{r drop_categories, results='hide'}
# Reduce categories on al and su -----------------------------------------------
# Transforming into numeric for easy replacement
data$al <- as.numeric(as.character(data$al))
data$su <- as.numeric(as.character(data$su))

# Change level: 5 -> 4
data$al[data$al == 5] <- 4
data$su[data$su == 5] <- 4

# Turn into factors (order) again without level 5
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4"),
                  ordered = TRUE)
data$su <- factor(data$su, 
                  levels = c("0", "1", "2", "3", "4"), 
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
table(data$al)
table(data$su)
```

## Identify missing data.

Next step focus on identify missing values on the dataset that can be due to various factors such as typo errors or unreported patient information.

This dataset contains some missing values and, their must be handled in order to use all machine learning model proposed during lecture. The main reason is that not all models support NA's. The below table shows the missing values by columns express in percentage. Because none of this variables exceed a threshold of $60$% of missing values, it is recommend to not delete any variable avoiding loosing this information.

```{r counting_NAs}
total_na <- sum(is.na(data)) # 1012 NAs

# Counting and sorting NA's
missing_values <- colSums(is.na(data))
sort_missing_values <- sort(missing_values, decreasing = TRUE)

# Counting and sorting NA's, display results in %
percentage_na <- missing_values/nrow(data)*100
sort(percentage_na, decreasing = TRUE)
```

First approach is to check if a patient record has not been well reported. It has been found $7$ observations where $11$ columns contains invalid data. These records are removed because, approximately, $45.83$% information of the patient is missing. Maintaining such rows would required data fabrication and will directly affect the future analysis making it weaken.

```{r rm_rows}
# Identifying observations with at least 1 NA ----------------------------------
obvs_with_na <- data[rowSums(is.na(data)) > 0, ]
missing_counts <- rowSums(is.na(obvs_with_na))

# df for storing number of NAs and corresponding indexes
missing_info <- data.frame(
  count = names(table(missing_counts)),
  indexes = tapply(names(missing_counts), missing_counts, paste, collapse = ", ")
)

pander(missing_info)

# Delete patients with 11 NA variables -----------------------------------------
rows_to_delete <- c(60, 87, 105, 149, 166, 223, 229)
data <- data[!rownames(data) %in% rows_to_delete, ]
```

## Split the dataset.

This step consist on divide the dataset into two subsets. The training set, takes $80$% of the data and it is used to train the models. This subset allows the model to learn patterns and relationships within the data. On the other hand, testing set ($20$% of the data) is used by unseen data to evaluate the performance of the trained model.

```{r split_dataset, results='hide'}
set.seed(1234) # For reproducibility

# Split into training (80%) and testing (20%) set
index <- createDataPartition(data$class, p=0.8, list=FALSE)
training <- data[ index,]
testing <- data[-index,]

nrow(training) # Number of observation for training
nrow(testing) # Number of observation for testing
```

## Handling missing data.

In this section, we cover the imputation of the missing data in the dataset using the MICE technique. This method is provided by the R library `mice` [@mice_ref] and on this script, we apply a slightly different version that allow us dealing with training and testing sets for avoiding data leakage.

Some important consideration on the configuration of this process are the election of 'rf' (random forest) method across all variables, in order to handle possible non-linear relationships. Moreover, we generate $4$ imputations for the training set. However, for this project, we will only consider the first imputation. After generating the imputed data in the training set, we can complete the missing values using the `complete()` function, which fills in missing values with those from the first imputation. Finally, and where we modify the classic version, we apply `mice.reuse()` function [@miceReuse_ref] which let us to impute the missing values in the testing set while avoiding data leakage. This function use the model developed from the training imputation and apply it on the testing set, thus ensuring greater reliability in the imputation of values through the use of learned patterns.

```{r mice_imputation, warning=FALSE, message=FALSE, results='hide'}
# MICE Imputation --------------------------------------------------------------
meth <- rep("rf", length = ncol(training)) # Set all methods to RF model

# Generates 4 imputations
mice_model <- mice(training,
                   m = 4, maxit = 5, method = meth,
                   seed = 1234, print = FALSE
                   )

# Complete the TRAIN data with the first imputation
training_imp <- complete(mice_model, 1)

# Function to impute new observations based on the previous imputation model
source("https://raw.githubusercontent.com/prockenschaub/Misc/master/R/mice.reuse/mice.reuse.R")

# Apply the imputation model to the TESTING set and extract the first imputation
testing_imp <- mice.reuse(mice_model, testing, maxit = 1)[[1]]
```

**Performance diagnostic of how the imputation is working**.

To check there are no missing data anymore, we use a graph from the `library(VIM)` [@VIM_ref], which display the percentage of NA's.

```{r performance_mice1, echo=FALSE, fig.align='center', fig.height=3, fig.width=4}
# Plot 1: Represent the % of missing data
aggr(training_imp,
     col = c("navyblue", "yellow"),
     numbers = TRUE,
     sortVars = FALSE,
     labels = names(training_imp),
     cex.axis = .7,
     gap = 3,
     ylab = c("Missing data", "Observed data")
)
```

Also, to assess whether the imputed data adequately reflects the distributions of the original data, we can use the `densityplot()` function. This function shows the marginal distribution of the observed data in blue, and in red, it shows the four densities for the predictors with initially missing data. As we can observed, the distributions across all variables fit properly following the original distribution. Hence, we can ensure the good quality of the MICE model for handling NAs.

```{r performance_mice2, echo=FALSE, fig.align='center', fig.height=4, fig.width=5}
# Plot 2: Check it follows the distribution of the original variable
densityplot(mice_model)
```

Finally, we can deeper explore the distribution of the imputed values across the first four imputations, compared to the observed data (represented by imputation number $0$). The red points are the imputed values which are consistently clustered across all iteration, suggesting again that the MICE algorithm is a reliable performance. Furthermore, this plot demonstrated how well the first distribution (the one we have chosen) is working.

```{r performance_mice3, echo=FALSE, fig.align='center', fig.height=4, fig.width=5}
# Plot 3: Visualize the imputed values for the 'age' variable across multiple imputations
# https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html
stripplot(mice_model, pch = 19, xlab = "Imputation number")
```

\pagebreak
# Exploratory Data Analysis (EDA)

The aim is to uncover patterns, detect outliers, and test assumptions through visual and quantitative methods.

## Numeric variables.

The `ggpair` function from the GGally package [@GGally_ref], allow us to create a scatter plot matrix for this variables. This matrix includes histograms for each variable along the diagonal, showcasing their distribution. The lower triangle displays scatter plots to explore the relationships between variable pairs, while the upper triangle shows the correlation coefficients, providing a quantitative measure of their relationships. This information allow us to identify the shape distribution, tendencies, outliers, among other things, thanks to different views in one grid.

```{r scatterPlot_contiuous, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
# Select continuous variables (include target)
scatterPlot_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc", "class")]

# Create scatter plot matrix
ggpairs(scatterPlot_data,
        aes(fill="pink"),
        lower = list(continuous = "points", combo = "box_no_facet"),
        upper = list(continuous = "cor"),
        diag = list(continuous = "barDiag"),
        title = "Scatter plot matrix"
       )
```

**General insights**.

1. This analysis reveals a non-normal distribution for the numeric variables. To improve the data distribution for machine learning methods, such as *Neural networks*, the data will be transformed depending on its skewness. This below table shows the skewness measures for each variable and it has been obtained from `psych` [@psych_ref] package. Log-transformation are applied to variables with positive skewness (`bp`, `bgr`, `bu`, `sc`, `pot` and `wbcc`), while a square transformation is used for `sod` with negative skewness. Notice that the threshold for decision-making is set at skewness values above $1$ or below $-1$.

```{r skewness_analysis, warning=FALSE, message=FALSE, echo=FALSE}
numeric_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")]
skewness <- round(skew(numeric_data), 1)
result_df <- data.frame(variable = colnames(numeric_data), skewness = skewness)
pander(result_df)
```

2. Standardization of Data. The data will be standardized to benefit models that are sensitive to the scale of the data, such as *K-Nearest Neighbors* and *Neural Networks*. This step will not affect negatively to the other models like *Gradient Boosting*, which are less sensitive to scaling.

3. About correlation coefficients. It is worth mentioning that this dataset contains high correlation between some variables, this value is identify when coefficients are grater than $0.7$ or lower than $-0.7$). For instance, packed cell volume (`pcv`) and hemoglobin (`hemo`) have a strong positive correlation coefficient of $0.869$. From a biologically point of view makes sense, both parameter are related with the amount of oxygen going through human blood. Hemoglobin is the protein in red blood cells that carries oxygen, while the packed cell volume represents the volume percentage of red blood cells in blood. Moreover, red blood cells count (`rbcc`) is also highly correlated with `hemo` ($0.770$) and `pcv` ($0.766$) indicating that if one of this parameter increase the other one will increase too.

4. Outliers. It is notably the presence of some outliers over the variables. This observations (referring to the outliers) represent individuals with extreme values probably due to underlying medical conditions or measurement errors. Notice that this observations may influence directly the model performance and the conclusion derived from the data analysis.

**Relation with the output**. A deeper exploration of the most correlated variables with the target, `class`, let some important insights on identify chronic kidney disease and healthy patients. First pattern obtained is that people with chronic kidney disease (class = `ckd`) tend to have lower values of hemoglobin (`hemo`), packed cell volume (`pcv`), and red blood cell count (`rbcc`) as indicated by the red density distributions. In contrast, healthy patients (class = `notckd`) have higher values for these parameters, as shown by blue density distribution. Moreover, boxplots demonstrated same insight as before, with median values of `hemo`, `pcv`, and `rbcc` being lower in the chronic kidney disease group. Last distinction observed on the boxplots is about on how the outliers exist just on unhealthy people distribution. This situation might highlight the complexity for diagnosis and identifying the chronic kidney disease due to weird and unexpected behavior of this patients. 

```{r deeperExploration, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
deeperExploration_vars <- training_imp[, c("hemo", "pcv", "rbcc", "class")]

# Most correlate variables separate by target
ggpairs(deeperExploration_vars,
        aes(color = class, fill = class), # Apply color based on 'class'
        lower = list(continuous = wrap("points", alpha = 0.5)), # Transparency
        upper = list(continuous = "cor"),
        diag = list(continuous = wrap("densityDiag", alpha = 0.5))
) + 
  theme(legend.position = "bottom")
```

**Data transformation code**.

```{r numeric_transf}
# Numeric transformation
vars_log_transf <- c('bp', 'bgr', 'bu', 'sc', 'pot', 'wbcc') # Note: No zeros on any var

########################## TRAINING dataset ##########################
training_transf <- training_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
training_transf <- training_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
training_transf[,c("sod")] <- (training_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])

########################## TESTING dataset ##########################
testing_transf <- testing_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
testing_transf <- testing_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
testing_transf[,c("sod")] <- (testing_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])
```

## Cualitative variables.

**Binary variables**. 

Some tables are display in order to understand better the dataset. 
First table indicates that most of the patients have normal levels of red blood cells (`rbc`) and pus cell (`pc`). In the second table, the majority of data is regarding people without pus cell clumps (`pcc`) and bacteria (`ba`). The third table focus on various conditions, including hypertension (`htn`), diabetes mellitus (`dm`), coronary artery disease (`cad`), pedal edema (`pe`), and anemia (`ane`). Hypertension and diabetes mellitus seems as the more common conditions among the patients. Meanwhile the other parameters are present in no more than $19$% of the people tested, thus these conditions might not be commontly within the population. Finally, most people of the dataset report having a good appetite.
In summary, the binary variables are not equally distributed on this dataset.

```{r eda_binary_tables, echo=FALSE, warning=FALSE, message=FALSE}
# Tables for distributions -----------------------------------------------------
# Create tables depending on the possible output
summary_abnormal <- lapply(c("rbc", "pc"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_notpresent <- lapply(c("pcc", "ba"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_noYes <- lapply(c("htn", "dm", "cad", "pe", "ane"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_poor <- lapply(c("appet"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)


# Print tables
pander(summary_abnormal, format = "html", escape = FALSE)
pander(summary_notpresent, format = "html", escape = FALSE)
pander(summary_noYes, format = "html", escape = FALSE)
pander(summary_poor, format = "html", escape = FALSE)
```

The following grid of bar charts aim to show the target distribution across each binary variable. When looking at parameter related with infections, such as `ba` (bacteria) and `pcc` (pus cell clumps), these are less frequent in the overall dataset, but when they do occur, they are predominantly found in patients with kidney disease (class = `ckd`). In addition, poor appetite seems to be a common symptom in patients with ckd. Hypertension (`htn`) and diabetes mellitus (`dm`) are variables where storage a high number of sick patients with this conditions.

```{r eda_binary_plot, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center'}
binary_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane")

# Bar plots by output ----------------------------------------------------------
# Reshape the df to long
long_data <- pivot_longer(training_transf, 
                          cols = binary_vars, 
                          names_to = "variable", 
                          values_to = "value")

# Combined plots with facet_wrap
p <- ggplot(long_data, aes(x = value, fill = as.factor(class))) + 
  geom_bar(position = "dodge", stat = "count") +
  facet_wrap(~variable, scales = "free_x") + 
  labs(y = 'Count', fill = 'Class') + 
  theme_minimal() + 
  theme(axis.text.x = element_text(hjust = 1),
        strip.text.x = element_text(face = "bold")) + 
  scale_fill_brewer(palette = "Set2")

print(p) # Display grid
```

**Category variables**.

Next step consist on explore the $3$ multi-state variables: specific gravity (`sg`), albumin (`al`), and sugar (`su`). Specific gravity clearly shows that levels $1.020$ and $1.025$ are the common levels for healthy people. Conversely, lower states are more prevalent among the presence of chronic kidney disease. On the other hand, healthy population is expect to have a level $0$ of albumin as well as a low level of sugar. Otherwise, levels indicates a higly link to this chronic disease.

```{r eda_category_distribution, echo=FALSE, fig.align='center'}
# Plot for Specific Gravity (sg)
plot_sg <- ggplot(training_transf, aes(x = as.factor(sg), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Specific Gravity by target",
       x = "Specific Gravity",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))

# Plot for Albumin (al)
plot_al <- ggplot(training_transf, aes(x = as.factor(al), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Albumin by target",
       x = "Albumin",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))  

# Plot for Sugar (su)
plot_su <- ggplot(training_transf, aes(x = as.factor(su), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Sugar by target",
       x = "Sugar",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))

# Arrange plots in a grid
grid.arrange(plot_sg, plot_al, plot_su, ncol = 2)
```

Additionally, in order to confirm the above insights some stacked bar chart has been code. This plots shows the proportion of people with and without chronic kindey disease across the different categories of these three multi-state variables. All patterns are confirmed.

```{r eda_category_proportion, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
category_data <- melt(training_transf,
                      id.vars = "class",
                      measure.vars = c("sg", "al", "su"))

# Create the combined plot using the melted data
ggplot(category_data, aes(x = value, fill = class)) + 
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("notckd" = "darkolivegreen3", "ckd" = "brown3")) +
  labs(title = "Relationship with class",
       x = "", y = "Proportion") +
  facet_wrap(~variable, scales = "free_x", nrow = 2) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.background = element_blank(),
        strip.text.x = element_text(size = 10))
```

## Correlation matrix.

The heatmap represents the strength and direction of the relationships between the variables in the dataset, where the color intensity indicates the magnitude of the correlation. Dark colors, that can be either blue (positive) or red (negative), suggest a stronger relationship between variables. A positive coefficient means that an increase (or decrease) in one variable is associated with an increase (or decrease) in the other. Conversely, a negative correlation indicated that as one variables increase, the other decrease. And, color in the middle are interpreted as weak correlations.

For dealing with mixed data type, the `hetcor` function from the `psych` package [@psych_ref] in R is used. This enable the computation of correlation across numeric, binary and multi-state variables as long as the qualitative variable are represented in factors. When coding this step some inconveniences have been encountered with `al` and `htn` variables. To address this issue, both parameters are treated numerically to facilitate its computation in the correlation analysis.

```{r corr_heatmap, warning=FALSE, message=FALSE, fig.align='center'}
training_modify <- training_transf

# Some transformation needed for handling problematic variables: al, htn
training_modify$al <- as.numeric(as.character(training_modify$al))
#contrasts(training_transf$htn) # no as 0 and yes as 1
training_modify$htn <- as.numeric(training_modify$htn)

# Works for mix variables: binary, multi-state and numeric
# Qualitative variables have to be in factor
cor_matrix <- hetcor(training_modify)
cor_values <- cor_matrix$correlations
corrplot(cor_values, method="color", type="upper", order="hclust",
         tl.cex=0.7, tl.col="black", tl.srt=45)
```

**Analytical exploration**.

Thanks to this exploration $17$ strong relationships have been discovered (the threshold for filter the correlations is set to coefficients higher than $0.7$ or lower than $-0.7$). However, I decided to focus on the top $8$ with the most significant impact, which can seen on the below table.

Most of this relationships are kind of intuitive. For example, having diabetes is closely linked to higher sugar levels, and a lower amount of hemoglobin seems to be associated with anemia. Overall, it is realised how related metabolomic and blood measures are when identifying chronic kidney disease.

```{r}
cor_flat <- as.data.frame(as.table(cor_values))
cor_flat <- cor_flat[order(-abs(cor_flat$Freq)),]

# Filter correlations: >0.7 or <-0.7
strong_correlations <- subset(cor_flat, abs(Freq) > 0.7 & Var1 != Var2)

# Remove duplicates rows
strong_correlations$pairID <- apply(strong_correlations[, c("Var1", "Var2")], 
                                    1,
                                    function(x) paste(sort(x), collapse = "-"))
strong_correlations <- strong_correlations[!duplicated(strong_correlations$pairID),]
strong_correlations$pairID <- NULL # Remove ID

# Add sign
strong_correlations$Sign <- ifelse(strong_correlations$Freq > 0, "+", "-")

#pander(strong_correlations) # 17 high realtionship
head(strong_correlations, 8) # Display the highest 8 relations
```

## Target variable.

To see the distribution of our target variable (`class`), we first look at the raw counts and then at the percentages to understand the proportion of sickness patients. To conclude, it is worth mentioning that the output varibale is not imbalanced, thereby there is no need for upsampling or downsampling this parameter.

```{r target_distribution}
# Check counts of chronic kidney disease
pander(table(data$class))

# Results in percentage
pander(prop.table(table(data$class)) * 100)
```

\pagebreak
# Machine-learning tools

The goal of this section is to evaluate the performance of some machine learning tools for the binary classification task. The aim is to predict whether the patient have chronic kidney disease or not. For each of these models, we will compute the accuracy, the kappa as well as the confusion matrix, a table which will indicate us how many instances were correctly classified and how many were misclassified. Finally, it is worth mentioning, that for this second part of the project are defined some common consideration that I will apply for all models implemented.

First, training and testing datasets are divided into features and the target variable.

```{r split_subsets}
# Divide into features and target both subsets
X_train <- training_transf %>% dplyr::select(-class)
y_train <- training_transf$class

X_test <- testing_transf %>% dplyr::select(-class)
y_test <- testing_transf$class
n_test <- dim(testing_transf)[1]
```

To ensure a robust evaluation, a train control parameter is configure: $5$-folds cross-validation repeated $10$ times for optimize hyper-parameters.

```{r train_ctrl_setting}
# Set up train control -> 5-folds cross validation repeated 10 times
ctrl <- trainControl(method = "repeatedcv",
                     number = 5, 
                     repeats = 10,
                     verboseIter = TRUE, # TRUE for tracking
                     classProbs = TRUE)
```

On the other hand, `caret` [@caret_ref] library's is the package used for code all the models. In order to understand the capabilities for each available models and their tunable parameters, the following commands has been used:

```{r caret_exploration_models, eval=FALSE}
# Models supported by caret
names(getModelInfo())

# In order to explore which parameter can be tune
name_caret_model <- "knn" # Change me!
modelLookup(name_caret_model)
```

Lastly, due to time consuming hyper-parameter search the best-performing models are pre-trained, saved, and stored in a dedicated folder (named pretrained_models). This not only saves time during the analysis but also ensure the reproducibility of results for the reader.

```{r folder_pretrain_mod, eval=FALSE, include=FALSE}
# Specify the folder path
folder_path <- "pretrained_models"
# Create the folder if it doesn't exist
if (!dir.exists(folder_path)) {
  dir.create(folder_path)
}
```

```{r loding_model_pretrain}
# Loading pre-trained models from pretrained_models folder
# KNN model --------------------------------------------------------------------
knn_fit <- readRDS("pretrained_models/knn_fit.rds")
# SVM model --------------------------------------------------------------------
svm_fit <- readRDS("pretrained_models/svm_fit.rds")
# DT model ---------------------------------------------------------------------
rpart_fit <- readRDS("pretrained_models/rpart_fit.rds")
c50_fit <- readRDS("pretrained_models/c50_fit.rds")
# RF model ---------------------------------------------------------------------
rf_fit_final <- readRDS("pretrained_models/rf_fit_final.rds")
# GB model ---------------------------------------------------------------------
xgb_fit <- readRDS("pretrained_models/xgb_fit.rds")
# NN model ---------------------------------------------------------------------
nn_fit <- readRDS("pretrained_models/nn_fit.rds")
# DNN model ---------------------------------------------------------------------
dnn_fit <- readRDS("pretrained_models/dnn_fit.rds")
```

## k-Nearest Neighbors

k-Nearest Neighbors classifier, commonly known as kNN, looks at the closest training examples in the feature space and uses a majority vote to determine the target. One of the advantages is that it makes no assumptions about the underlying data distribution which make it perfect for capturing non-linear data.

The main parameter of kNN is $k$, which allow us to set the number of neighbors to consider for obtaining an optimal accuracy. After a search of this hyper-parameter, the best performance was achieved when $k = 3$, indicating that the model performs right when making predictions based on the three closest neighbors.

```{r hide_train_knn, eval=FALSE}
# Hyper-parameter search -------------------------------------------------------
# K no more than 50% of the training set size.
# From lecture: preferable to try odd numbers for k
knn_grid <- expand.grid(k = seq(3, 121, by = 2))

# Training the model -----------------------------------------------------------
knn_fit <- train(class ~ .,
                 method = "knn",
                 data = training_transf,
                 tuneGrid = knn_grid,
                 metric = "Accuracy",
                 trControl = ctrl)
print(knn_fit)
# Best values: k = 3

# Save model -------------------------------------------------------------------
saveRDS(knn_fit, file = "pretrained_models/knn_fit.rds")
```

```{r conf_matrix_knn}
# Predictions on test ----------------------------------------------------------
knn_pred <- predict(knn_fit, X_test)
knn_pred_prob <- predict(knn_fit, X_test, type = "prob")

# Performance measures ---------------------------------------------------------
confusionMatrix(knn_pred, y_test)
```

When unseen data is used with the best model obtained (hyper-parameter already selected), the model's performance achieved an accuracy of $88.46$%, with a $95$% confidence interval ranging from $79.22$% to $94.59$%. As it is expected, the Kappa metric is slightly lower, this measure considers the accuracy that could be achieved by random chance, making it a more robust metric than accuracy alone.

Furthermore, by analyzing the confusion matrix we can understand the model's mistakes. There are no false positives, which is an advantage taking into account this type of error can be costly for the healthcare center. However, the worst mistake, from a life standpoint, occurs when predicting no disease but the patient actually has it. And this is where all errors are concentrated.

**Performance of the model**.

The hyper-parameter tuning plot illustrates the relationship between the number of neighbors and the model's performance metrics, accuracy as well as kappa. While $k$ increases, this two metrics decrease which lead us that too many neighbors make the model less accurate. The highest metric values are reached when $k = 3$.

```{r knn_measures1, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot hyper-parameter tuning --------------------------------------------------
knn_results_search <- knn_fit$results

# df to long format
knn_results_long <- melt(knn_results_search,
                     id.vars = "k", 
                     measure.vars = c("Accuracy", "Kappa"),
                     variable.name = "Metric", value.name = "Value")

# Plotting both metrics
ggplot(knn_results_long, aes(x = k, y = Value, color = Metric)) + 
  geom_line() + 
  geom_point() +
  scale_color_manual(values = c("Accuracy" = "dodgerblue4", "Kappa" = "darkorange3")) +
  xlab("Number of neighbors (k)") + 
  ylab("Metric value") +
  ggtitle("KNN tuning vs. Number of neighbors") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank()) +
  theme(legend.position = "bottom")
```

The variable importance plot for the k-Nearest Neighbors model displays the relative importance of each predictor in the classification task. As it can be observed, hemoglobin (`hemo`) stand out as the most significant variable when forecasting the presence or absence of chronic kidney disease. Nevertheless, we cannot ignore packed cell volume (`pcv`), specific gravity (`sg`), serum creatinine (`sc`) or red blood cell count (`rbcc`) which still being crucial for model decision-makers.

```{r knn_measures2, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot importance on training --------------------------------------------------
knn_var_imp <- varImp(knn_fit, scale=FALSE)
plot(knn_var_imp, main = "Variable importance for KNN")
```

## Support Vector Machines

Support Vector Machines, known also as SVM, is a classifier based on the idea of maximizing the gap between different classes. Kernel functions plays an important role on this models and works well with non-linear problems. Additionally, is not influenced by the presence of outliers which can be beneficial for this dataset and its well-known for reducing the risk of overfitting. In this analysis, we have utilized the *radial basis function kernel* in caret (`svmRadial`).

```{r hide_train_svm, eval=FALSE}
# Hyper-parameter search -------------------------------------------------------
# C; compromising between low errors and weights values
# sigma; determine how far the influence of a single training example reaches
svm_grid <- expand.grid(C =  c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4), 
                        sigma = c(0.01, 0.05, 0.1, 0.5, 1, 2, 5)
                        )

# Training the model -----------------------------------------------------------
svm_fit <- train(class ~., method = "svmRadial", 
                data = training_transf,
                tuneGrid = svm_grid, 
                metric = "Accuracy",
                trControl = ctrl)

print(svm_fit)
# Best values: sigma = 0.5 and C = 1

# Save model -------------------------------------------------------------------
saveRDS(svm_fit, file = "pretrained_models/svm_fit.rds")
```

```{r conf_matrix_svm}
# Predictions on test ----------------------------------------------------------
svm_pred <- predict(svm_fit, X_test)
svm_pred_prob <- predict(svm_fit, X_test, type = "prob")

# Performance measures ---------------------------------------------------------
confusionMatrix(svm_pred, y_test)
```

The SVM model's performance reached incredible results metrics: an accuracy of $96.15$% and a kappa of $91.72$%, which represent almost a perfect model. The confusion matrix reveals that there has not been done any **worst mistake** (predicting the patient does not have chronic kidney disease when he actually has). However, there are a few times where the model forecast that the patient has ckd and in reality the patient is healthy.

**Performance of the model**.

During the training phase, we conducted a hyper-parameter search to balance the trade-off between ensuring low error (parameter `C`) and managing the influence range of support vectors (`sigma`). From all values tried the model performs best with $sigma = 0.5$ and $C = 1$.

The parameter search space plot, suggest that smaller values of sigma lead us into better models. Moreover, with the appropriate sigma it seems the choice of $C$ is less critial to the model's performance.

```{r svm_measures1, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot hyper-parameter tuning --------------------------------------------------
selected_sigma_values <- unique(svm_fit$results$sigma)
svm_results_filtered <- subset(svm_fit$results, sigma %in% selected_sigma_values)

ggplot(svm_results_filtered, aes(x=log10(C), y=Accuracy, color=factor(sigma))) + 
  geom_line() + 
  geom_point() +
  xlab("log10(C)") + 
  ylab("Accuracy") +
  ggtitle("Accuracy vs. C for selected sigma Values") +
  scale_color_discrete(name = "sigma") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank())
```

The variable importance plot highlights again hemoglobin (`hemo`) as the most significant variable when forecasting the presence or absence of chronic kidney disease. Other important parameters include packed cell volume (`pcv`), specific gravity (`sg`) or serum creatinine (`sc`). As an insight, SVM aligns with KNN in terms of the key variables, the only difference is in their ranking for `pe` and `rbc` variables.

```{r svm_measures2, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot importance on training --------------------------------------------------
svm_var_imp <- varImp(svm_fit, scale=FALSE)
plot(svm_var_imp, main = "Variable importance for SVM")
```

## Models based on decision trees

### Decision trees

In general, decision trees are well suited for dataset with missing values or outliers on it. Although this dataset not longer contains NA's, using decision trees can still be advantageous due to the presence of outliers in some of the variables. Furthermore, their ability for automatic feature selection make it easy its interpretability. For this kind of model we explore two options provided by the `caret` library.

**RPART MODEL**. The RPART method, is a type of decision trees which offers simplicity and interpretability without supporting capability for boosting.

```{r hide_train_rpart, eval=FALSE}
# Hyper-parameter search -------------------------------------------------------
# cp; size of the tree
rpart_grid <- expand.grid(.cp = c(0.01, 0.05, 0.1))

# Training the model -----------------------------------------------------------
rpart_fit <- train(class ~.,
                   method = "rpart",
                   data = training_transf,
                   tuneGrid = rpart_grid,
                   metric = "Accuracy",
                   trControl = ctrl)

print(rpart_fit)
# Best values: cp = 0.01

# Save model -------------------------------------------------------------------
saveRDS(rpart_fit, file = "pretrained_models/rpart_fit.rds")
```

```{r conf_matrix_rpart}
# Predictions on test ----------------------------------------------------------
rpart_pred <- predict(rpart_fit, X_test)
rpart_pred_prob <- predict(rpart_fit, X_test, type = "prob")

# Performance measures ---------------------------------------------------------
confusionMatrix(rpart_pred, y_test)
```

The evaluation of this model with the testing set achieved almost a perfect mode where the accuracy is about $97.44$% and the kappa is $94.58$%. The model's confusion matrix revealed that the missclasification is minimum, with only two incorrect predictions (once for each type of error).

**Performance of the model**.

Same accuracy and kappa metrics has been obtained across different complexity parameters (`cp`). Therefore, the visual representation of the tuning process is omitted as it does not enhance the understanding of the model's performance. Instead, I will focus on the decision tree diagram coding thanks to the `rattle` package [@rattle_ref]. This interesting plot, illustrate the decision-making process of the model. It start with hemoglobin as the primary node for splitting the data with a threshold of $0.17$. Then the second node, based it decision on `sg` variable.

```{r rpart_measures1, eval=FALSE}
# Plot tuning hyper-parameter --------------------------------------------------
plot(rpart_fit, main = 'Hyper-parameter tuning - DT rpart')
```

```{r rpart_measures2, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# DT visualization -------------------------------------------------------------
fancyRpartPlot(rpart_fit$finalModel)
```

In terms of variable importance, the **rpart** model has selected $7$ variables out of $24$ as relevant parameters. This represent less than a $30$% of the columns on the dataset. However, once again `hemo` belong to the top $3$ more important variables.

```{r rpart_measures3, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot importance on training --------------------------------------------------
rpart_var_imp <- varImp(rpart_fit, scale=FALSE)
plot(rpart_var_imp, main = "Variable importance for DT rpart")
```

**TREE MODEL**. The second model coded is the C5.0 algorithm which is an advanced *decision tree* that can be considered as a form of boosting approach.

```{r hide_train_c50, eval=FALSE}
# Hyper-parameter search -------------------------------------------------------
# .winnow; feature selection step conducted before modelling
# .trial; number of boosting iterations
c50_grid <- expand.grid( .winnow = c(TRUE, FALSE), 
                         .trials=c(1, 5, 4),
                         .model= "tree"
                         )

# Training the model -----------------------------------------------------------
c50_fit <- train(class ~., 
                 method = "C5.0",
                 data = training_transf,
                 tuneGrid = c50_grid,
                 metric = "Accuracy",
                 trControl = ctrl)

print(c50_fit)
# Best values: trials = 1, model = tree and winnow = FALSE

# Save model -------------------------------------------------------------------
saveRDS(c50_fit, file="pretrained_models/c50_fit.rds")
```

```{r conf_matrix_c50}
# Predictions -----------------------------------------------------------------
c50_pred <- predict(c50_fit, X_test)
c50_pred_prob <- predict(c50_fit, X_test, type = "prob")

# Performance measures ---------------------------------------------------------
confusionMatrix(c50_pred, y_test)
```

Upon application to the testing set, the C5.0 model achieved the same results to the previously decision tree model, the *rpart*. New data, compute same confusion matrix, accuracy and kappa values. Hence, we can conclude there is no different on how rpart and C5.0 are performing when unseen data is introduced. Even though this evaluation metrics are the same, let's explore the differences between this two models when being created.

**Performance of the model**.

The model tuning revealed that configuring the winnowing parameter as FALSE yields consistently into better results, independently the number of iteration used.

```{r c50_measures1, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot hyper-parameter tuning --------------------------------------------------
plot(c50_fit, main = 'Hyper-parameter tuning - DT C5.0')
```

Variable importance for the C5.0 model shows a limited set of variables, with only four out of twenty four columns being significant. Hemoglobin (`hemo`) is the most critical feature and specific gravity (`sg`) follows as the second most important. The model also gives some relevance to blood glucose random (`bgr`) and albumin (`al`), however in a minor degree.

```{r c50_measures2, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot importance on training --------------------------------------------------
c50_var_imp <- varImp(c50_fit, scale=FALSE)
plot(c50_var_imp, main="Variable importance for DT C5.0")
```

### Random Forests

Random forest are an evolution of decision tree, designed for enhancing model's performance by reducing overfitting. It involves creating multiple decision trees and aggregate the predictions of the trees in order to produce the final prediction. `caret` package only allow to tune the `mtry` parameter however I want to illustrate that it is possible to expand the search manually to find other parameters such as the `ntree` variable.

```{r hide_train_rf, eval=FALSE}
# Hyper-parameter search -------------------------------------------------------
# mtry; nº of variables randomly choose for building the trees
# Common practice for the grid is [1, sqrt(n_variables)]
rf_grid <- expand.grid(mtry = c(1, 2, 3, 4, 5))

# Training the model -----------------------------------------------------------
rf_fit <- train(class ~.,
                method = "rf",
                data = training_transf,
                ntree = 200,
                tuneGrid = rf_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(rf_fit)
# Best values: mtry = 2

# Try different ntree values manually ------------------------------------------
model_rf_list <- list()
# Values I want to test (my search-space)
testing_ntree_var <- seq(200, 1000, by = 100)

for (ntree in testing_ntree_var){
  fit <- train(class~.,
               method = 'rf',
               data = training_transf,
               ntree = ntree,
               tuneGrid = rf_grid,
               metric = 'Accuracy',
               trControl = ctrl)
  key <- toString(ntree)
  model_rf_list[[key]] <- fit
}

results_rf_manually <- resamples(model_rf_list)
summary(results_rf_manually)
dotplot(results_rf_manually)
# Best values: ntree = 1000

# Final search with ntree=1000 -------------------------------------------------
rf_fit_final <- train(class ~.,
                      method = "rf",
                      data = training_transf,
                      ntree = 1000,
                      tuneGrid = rf_grid,
                      metric = "Accuracy",
                      trControl = ctrl)

print(rf_fit_final)

# Save model -------------------------------------------------------------------
saveRDS(rf_fit_final, file = "pretrained_models/rf_fit_final.rds")
```

```{r conf_matrix_rf}
# Predictions on test ----------------------------------------------------------
rf_predF <- predict(rf_fit_final, X_test)
rf_predF_prob <- predict(rf_fit_final, X_test, type = "prob")

# Performance measures ---------------------------------------------------------
confusionMatrix(rf_predF, y_test)
```

When testing the final model on the test set, the random forest algorithm showed the most promising results until now, reflecting the strength of the learning. Indeed, the accuracy as well as the kappa is of $1$. Which might indicated that this model required a most complex dataset or/and the model is overfitting to the test data.

**Performance of the model**.

The visualization of hyper-parameter tuning is limited to the `mtry`. And we can observed an increase in accuracy when going from one to two predictors, indicating that considering at least two features at each split significantly enhances the model's. Also, it is clear than a higher values does not improve the model neither.

```{r rf_measures1, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot hyper-parameter tuning --------------------------------------------------
plot(rf_fit_final, main = 'Hyper-parameter tuning - RF')
```

The variable importance plot again highlights hemoglobin (`hemo`) as a primary feature, consistent with earlier findings using other models. This reinforces the thought that hemoglobin levels are a probably the most important biomarker in the diagnosis of chronic kidney disease. The model also identifies other key variables that contribute significantly such as `pcv` or `sc`.

```{r rf_measures2, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot importance on training --------------------------------------------------
rf_var_imp <- varImp(rf_fit_final, scale=FALSE)
plot(rf_var_imp, main="Variable importance for RF")
```

### Gradient Boosting

```{r}
# Hyper-parameter search -------------------------------------------------------
#https://csantill.github.io/RTuningModelParameters/
xgb_grid = expand.grid(
  nrounds = c(100, 500),
  eta = c(0.01, 0.1), # Learning rate
  max_depth = c(3, 6), # Tree depth
  gamma = c(0, 1),
  colsample_bytree = c(0.5, 0.8), # Feature sampling
  min_child_weight = c(1, 5),
  subsample = c(0.7, 0.9) 
)
# dim(xgb_grid) # 128 combinaciones 
# Training the model -----------------------------------------------------------
xgb_fit = train(class ~.,
                data = training_transf,
                tuneGrid = xgb_grid,
                metric = "Accuracy",
                method = "xgbTree",
                trControl = ctrl
                )

print(xgb_fit)
# Best values: nrounds = 100, max_depth = 6, eta = 0.1, gamma = 1, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.9

# Save model -------------------------------------------------------------------
saveRDS(xgb_fit, file="pretrained_models/xgb_fit.rds")

# Predictions -----------------------------------------------------------------
xgb_pred = predict(xgb_fit, X_test)
xgb_pred_prob = predict(xgb_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(xgb_pred, y_test)
```

## Neural Networks

```{r warning=FALSE, message=FALSE}
# Hyper-parameter search -------------------------------------------------------
# size; nº of neurons in the hidden layer
# decay
nn_grid <- expand.grid(size=c(2,4,6), 
                       decay=c(0.01,0.001))

# Training the model -----------------------------------------------------------
# NN with 1 hidden layer
nn_fit <- train(class ~.,
                method = "nnet",
                data = training_transf,
                MaxNWts = 1000,
                maxit = 100,
                tuneGrid = nn_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(nn_fit)
# Best values: size = 6 and decay = 0.001

# Save model -------------------------------------------------------------------
saveRDS(nn_fit, file="pretrained_models/nn_fit.rds")

# Predictions -----------------------------------------------------------------
nn_pred = predict(nn_fit, X_test)
nn_pred_prob = predict(nn_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(nn_pred, y_test)
```

```{r nn_measures, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot tuning hyper-parameter --------------------------------------------------
plot(nn_fit)

# Plot importance on training --------------------------------------------------
nn_imp <- varImp(nn_fit, scale = F)
plot(nn_imp,
     scales = list(y = list(cex = .95)), 
     main="Variable importance for Neural Networks")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
nn_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = nn_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = nn_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with NN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_nn <- roc(y_test, nn_pred_prob[,2])
plot(roc_nn, main="ROC Curve for Optimal NN Model")
```

## Deep Neural Network

```{r warning=FALSE, message=FALSE}
# Hyper-parameter search -------------------------------------------------------
# layer1, layer2, layer3: nº of neuron in each hidden layers
dnn_grid <- expand.grid(layer1 = c(5, 10, 15),
                        layer2 = c(0, 5, 10),
                        layer3 = c(0, 5),
                        hidden_dropout = c(0.1, 0.2), #dense neuron network
                        visible_dropout = c(0.1, 0.2))
# dim(dnn_grid) # 72 combinations

# Training the model -----------------------------------------------------------
# NN with 1 hidden layer
dnn_fit <- train(class ~.,
                method = "dnn",
                data = training_transf,
                numepochs = 20, # nº of iterations on the whole training set
                tuneGrid = dnn_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(dnn_fit)
# Best values:

# Save model -------------------------------------------------------------------
saveRDS(dnn_fit, file="pretrained_models/dnn_fit.rds")

# Predictions -----------------------------------------------------------------
dnn_pred = predict(dnn_fit, X_test)
dnn_pred_prob = predict(dnn_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(dnn_pred, y_test)
```

```{r}
# Plot tuning hyper-parameter --------------------------------------------------
plot(dnn_fit)

# Plot importance on training --------------------------------------------------
dnn_var_imp <- varImp(dnn_fit, scale = FALSE)
plot(dnn_var_imp,
     scales = list(y = list(cex = .95)), 
     main="Variable importance for Deep Neural Netwrok")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
dnn_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = dnn_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = dnn_test_plot, 
       aes(x = Individual, y = Probability_CKD, color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with DNN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_dnn <- roc(y_test, dnn_pred_prob[,2])
plot(roc_dnn, main="ROC Curve for Optimal DNN Model")
```

\pagebreak
# Overview models

```{r}
# Compare model performance ----------------------------------------------------
models_compare <- resamples(list(KNN = knn_fit,
                                 SVM = svm_fit,
                                 DT.rpart = rpart_fit,
                                 DT.c50 = c50_fit,
                                 RF = rf_fit_final,
                                 xGB = xgb_fit,
                                 NN = nn_fit,
                                 DNN = dnn_fit)
                            )

# Summary
summary(models_compare)

# Box-and-Whisker Plot
bwplot(models_compare, metric = "Accuracy",
       main = "Model comparison based on Accuracy")

# Dotplot
dotplot(models_compare, metric = "Accuracy",
        main = "Model Comparison based on Accuracy")
```

```{r warning=FALSE, message=FALSE}
# KNN ROC ----------------------------------------------------------------------
roc_knn <- roc(y_test, knn_pred_prob[,2])
plot(roc_knn, main = "ROC Curve for Optimal KNN Model")

# KNN plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
kkn_test_plot <- data.frame(
  Probability_Prob = knn_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = kkn_test_plot, aes(x = Actual_Class, 
                                 y = Probability_Prob, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with KNN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")



# SVM ROC ----------------------------------------------------------------------
roc_svm <- roc(y_test, svm_pred_prob[,2])
plot(roc_svm, main = "ROC Curve for Optimal SVM Model")

# SVM plot testing performance -------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
svm_test_plot <- data.frame(
  Probability_Prob = svm_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = svm_test_plot, aes(x = Actual_Class, 
                                 y = Probability_Prob, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with SVM") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")



# DT-rpart ROC -----------------------------------------------------------------
roc_rpart <- roc(y_test, rpart_pred_prob[,2])
plot(roc_rpart, main = "ROC Curve for Optimal DT-rpart Model")

# DT-rpart plot testing performance --------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
rpart_test_plot <- data.frame(
  Probability_Prob = rpart_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = rpart_test_plot, aes(x = Actual_Class,
                                   y = Probability_Prob,
                                   color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with DT-rpart") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")



# DT-c5.0 ROC ------------------------------------------------------------------
roc_c50 <- roc(y_test, c50_pred_prob[,2])
plot(roc_c50, main="ROC Curve for Optimal DT-C5.0 Model")

# DT-c5.0 plot testing performance --------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
c50_test_plot <- data.frame(
  Probability_Prob = c50_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = c50_test_plot, aes(x = Actual_Class,
                                 y = Probability_Prob,
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with DT-C5.0") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")



# RF ROC -----------------------------------------------------------------------
roc_rf <- roc(y_test, rf_predF_prob[,2])
plot(roc_rf, main="ROC Curve for Optimal RF Model")

# RF plot testing performance --------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
rf_test_plot <- data.frame(
  Probability_Prob = rf_predF_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = rf_test_plot, aes(x = Actual_Class,
                                y = Probability_Prob,
                                color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with RF") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")
```


library(pdp) this library allow us to see the dependencies/Relation between variables. For example, we see with the first line of code that the dependency is not-lineal.
In LR the relation was a line, and it was capable to capture this relation.

```{r}
# Explore the relationship of the most important variable
library(pdp)
partial(rf_fit_final, pred.var = "hemo", which.class=2, plot = TRUE, prob=TRUE, rug = TRUE)
# Se pueden analizar más
```


\pagebreak
# Social impact

2º. **Economic assumption used**. Incorporating a custom metric in Caret.

- If the doctor correctly diagnose a patience does not have a chronic kidney disease, the 

**Economic assumption used**.

- If the company predicts a customer is going to stay and it actually stay, the company gains a 12% profit at the end of the quarter.

- If the company predicts a customer is going to stay but he leaves, the loss is 100%.

- If the company predict a customer is going to leave but he actually stay, the profit is 10%.

- If the company predict a customer is going to leave and he actually leave, then the profit is 3%.

Using the specified profits and losses for each prediction outcome, we compute the average profit per customer based on the model's predictions.

```{r eval=FALSE}
# lev=NULL is mandatory to get the k. So caret can interprretate this is the accuracy (or somthing like that ha dicho)
profit_unit <- c(0.12, -0.01, -1.0, 0.0)
EconomicProfit <- function(data, lev = NULL, model = NULL) 
{
  y_pred = data$pred 
  y_true = data$obs
  CM = confusionMatrix(y_pred, y_true)$table
  out = sum(profit_unit*CM)/sum(CM)
  names(out) <- c("EconomicProfit")
  out
}
```

\pagebreak
# References

<div id="refs"></div>
