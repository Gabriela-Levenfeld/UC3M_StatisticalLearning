---
title: 'Project 2: Machine-learning tools'
author: "Gabriela Levenfeld Sabau"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes   
    toc: TRUE              
    toc_float: TRUE
  geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
  mathjax: local
  self_contained: false
subtitle: MS in Statistics for Data Science
bibliography: references.bib
nocite: '@*'
link-citations: true
linkcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Dataset description.

Dataset obtained from **UC Irvine Machine Learning Repository** [@misc_chronic_kidney_disease_336].

## Variables description.
**Quantitative**
bp		-	Blood pressure
bgr		-	Blood glucose random
bu		-	Blood urea
sc		-	Serum creatinine
sod		-	Sodium
pot		-	Potassium
hemo	-	Hemoglobin
pcv		- Packed cell volume
wbcc	-	White blood cell count
rbcc	-	Red blood cell count

**Qualitative, Categorical**
All this $3$ variables has a order.
sg		-	Specific gravity
al		- Albumin
su		-	Sugar

**Qualitative, Binary**
rbc		-	Red blood cells
pc		-	Pus cell
pcc		-	Pus cell clumps
ba		-	Bacteria
htn		-	Hypertension
dm		-	Diabetes mellitus
cad		-	Coronary artery disease
appet	-	Appetite
pe		-	Pedal edema
ane		-	Anemia

**Target**
class	-	class

\pagebreak
# Data preprocessing

This section focuses on preparing the dataset for exploratory data analysis (EDA) and applying the computational models. This include loading the necessary material, handling missing values, encoding categorical variables, and splitting the dataset into training and testing subsets, among other tasks.

## Load material.

**Load libraries**. Several R libraries are needed for the project in order to work properly.

```{r load_libraries, message=FALSE, warning=FALSE}
library(skimr) # Summary statistics
library(tidyverse) # Include ggplot2, dplyr, among others
library(pander)
library(caret) # Models
library(mice) # Imputation missing data
library(VIM) # Plots for imputation perfoormance
library(GGally) # Plots of the continuous variables
library(psych) # Compute the skewness and correlation matrix
library(polycor) # Compute correlation heatmap mix data type
#library(patchwork) # Plot grid
library(gridExtra) # For grid.arrange
library(corrplot) # Correlation matrix
library(reshape2) # Data reshaping (melt)
library(pROC)
```

**Load dataset**. The dataset was stored in an .arff format. To open the file, I made some arrangements using Python enviroment, I attached the code used in order to generate the .csv so I can work with during the rest of the project.

```{python open_file_python, eval=FALSE}
# Require libraries ------------------------------------------------------------
from scipy import io
import pandas as pd

# Load .arff file: extract the df and metadata ---------------------------------
dataframe, meta = io.arff.loadarff('data/chronic_kidney_disease_full.arff')
# Transform into pd df for handling
dataframe = pd.DataFrame(dataframe) # Some variables are into b'value' (bad format)

# Fixing bad format ------------------------------------------------------------
def bytes_to_int(byte_value):
    try:
        return byte_value.decode('utf-8') # Decode bytes to a string
    except AttributeError:
        return byte_value # When it is already a string
      
# Apply the conversion function to the specified columns in df
for column in dataframe.columns:
    dataframe[column] = dataframe[column].apply(bytes_to_int)

# Save the df to .csv file -----------------------------------------------------
dataframe.to_csv("data/chronic_kidney_disease_full.csv", index=False)
```

```{r load_dataset}
# Load new .csv dataset
data <- read.csv("data/chronic_kidney_disease_full.csv", header = TRUE, sep = ",")
```

**First look at the dataset**. Brief overview of the dataset for further preprocessing steps.

```{r small_EDA1, eval=FALSE, include=FALSE}
# Small Exploratory Data Analysis
skim(data)
# Presents results for every column; the statistics it provides depend on the class of the variable

# For visualizing the first 2 rows
head(data, 2)
```

```{r small_EDA2}
glimpse(data)
```

## Data cleaning and Feature engineering.

**Ensure consistency on the dataset**. Missing values are represented sometimes as "?" and other times as NA. To ensure consistency over the whole data, it is converted all missing values into the same format. 

```{r consistency_dataset}
# Convert "?" to NA
data <- data.frame(lapply(data, function(x) ifelse(x == "?", NA, x)))
```


**Duplicate data**. There is no duplicated data.

```{r duplicate_data}
duplicates <- data[duplicated(data), ]
```

**Encoding qualitative variables**. The goal is to get a dataset that our statistical models knows how to manage. This include, turn it into factor the categorical variables. Meanwhile, binary encoding is easy, for the multi-state variables strategy is slightly different due to the presence of natural order. This means, that a higher lever of this variables are directly relevant for medical conditions, clearly affecting to medical conditions such as chronic kidney disease. This variables `sg`, `al` and `su` represent the specific gravity, albumin and sugar. In order to make the proper encoding, it is mandatory to specify the importance of the order when creating this levels using the factor function.

```{r data_cleaning, results='hide'}
# Binary variables -------------------------------------------------------------
# (Included the target variable)
bin_and_target_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane", "class")

for (var in bin_and_target_vars) {
  data[[var]] <- as.factor(data[[var]])
}

# Categorical variables --------------------------------------------------------
data$sg <- factor(data$sg, 
                  levels = c("1.005", "1.010", "1.015", "1.020", "1.025"), 
                  ordered = TRUE)
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4", "5"), 
                  ordered = TRUE)
data$su <- factor(data$su,
                  levels = c("0", "1", "2", "3", "4", "5"),
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
str(data) # To check all qualitative vars are in factor type
levels(data$sg) # Check the levels of a specific variable
table(data$al) # Check how many observation per level there is
```

**Drop categories on `al` and `su`**. `al` level $5$ contains just one observation, due to its low presence on the dataset it can be changed into level $4$ gaining stability and avoiding possible futures problems when imputing missing values. Same startegy is followed on `su` $4$ level. This approach assumes that the distinction between levels $4$ and $5$ is not significant.

```{r}
table(data$sg)
table(data$al)
table(data$su)
```

```{r drop_categories, results='hide'}
# Reduce categories on al and su -----------------------------------------------
# Transforming into numeric for easy replacement
data$al <- as.numeric(as.character(data$al))
data$su <- as.numeric(as.character(data$su))

# Change level: 5 -> 4
data$al[data$al == 5] <- 4
data$su[data$su == 5] <- 4

# Turn into factors (order) again without level 5
data$al <- factor(data$al, 
                  levels = c("0", "1", "2", "3", "4"),
                  ordered = TRUE)
data$su <- factor(data$su, 
                  levels = c("0", "1", "2", "3", "4"), 
                  ordered = TRUE)

# Checkings --------------------------------------------------------------------
table(data$al)
table(data$su)
```

## Identify missing data.

Next step focus on identify missing values on the dataset that can be due to typo errors, absence of this patient answers, etc.

This dataset contains missing values, we must handle them in order to use the models (not all models support NA's). The above table shows the missing values by columns express in percentage. None of this variables superan a threshold of $60$% of missing values thus it is recommend to not delete any variable avoiding loosing this information.

```{r counting_NAs}
total_na <- sum(is.na(data)) # 1012 NA

# Counting NA's ----------------------------------------------------------------
missing_values <- colSums(is.na(data))
sort_missing_values <- sort(missing_values, decreasing = TRUE)

percentage_na <- missing_values/nrow(data)*100 # NA's in %
sort(percentage_na, decreasing = TRUE)
```

**Delete patient information**. First approach is to take a look if there is a chance that data from a patient has not been well reported. There are $10$ observations that has $7$ columns with invalid data. These rows are deleted, there is no sense on make up around the $45.83$% of information of a patient.

```{r rm_rows}
# All observations with at least 1 NA ------------------------------------------
obvs_with_na <- data[rowSums(is.na(data)) > 0, ]
missing_counts <- rowSums(is.na(obvs_with_na))

# df for storing number of NAs and corresponding indexes
missing_info <- data.frame(
  count = names(table(missing_counts)),
  indexes = tapply(names(missing_counts), missing_counts, paste, collapse = ", ")
)

pander(missing_info)

# Delete patients with 11 NA variables -----------------------------------------
rows_to_delete <- c(60, 87, 105, 149, 166, 223, 229)
data <- data[!rownames(data) %in% rows_to_delete, ]
```

## Split the dataset.

This step consist on divide the dataset into two subsets. The training set, takes $80$% of the data and it is used to train the models. This subset allows the model to learn patterns and relationships within the data. On the other hand, testing set ($20$% of the data) is used by unseen data to evaluate the performance of the trained model.

```{r split_dataset, results='hide'}
set.seed(1234) # For reproducibility

# Split into training (80%) and testing (20%) set
index <- createDataPartition(data$class, p=0.8, list=FALSE)
training <- data[ index,]
testing <- data[-index,]

nrow(training) # Number of observation for training
nrow(testing) # Number of observation for testing
```

## Handling missing data.

```{r mice_imputation, warning=FALSE, message=FALSE, results='hide'}
# MICE Imputation --------------------------------------------------------------
meth <- rep("rf", length = ncol(training)) # Set all methods to RF model

# Generates 4 imputations
mice_model <- mice(training,
                   m = 4, maxit = 5, method = meth,
                   seed = 1234, print = FALSE
                   )

# Complete the train data with the first imputation
training_imp <- complete(mice_model, 1)

# Function to impute new observations based on the previous imputation model
source("https://raw.githubusercontent.com/prockenschaub/Misc/master/R/mice.reuse/mice.reuse.R")

# Apply the imputation model to the testing set and extract the first imputation
testing_imp <- mice.reuse(mice_model, testing, maxit = 1)[[1]]
```

**Perform diagnostics of how the imputation is working**.

```{r performance_mice1, fig.align='center', fig.height=3, fig.width=4}
# Plot 1: Represent the % of missing data
aggr(training_imp,
     col = c("navyblue", "yellow"),
     numbers = TRUE,
     sortVars = FALSE,
     labels = names(training_imp),
     cex.axis = .7,
     gap = 3,
     ylab = c("Missing data", "Observed data")
)
```


```{r performance_mice2, fig.align='center', fig.height=4, fig.width=5}
# Plot 2: Check it follows the distribution of the original variable
densityplot(mice_model)

# Plot 3: Visualize the imputed values for the 'age' variable across multiple imputations
# https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html
stripplot(mice_model, pch = 19, xlab = "Imputation number")
```

\pagebreak
# Exploratory Data Analysis (EDA)

The aim is to uncover patterns, detect outliers, and test assumptions through visual and quantitative methods.

## Numeric variables.

The `ggpair` function from the GGally package [@GGally_ref], allow us to create a scatter plot matrix for this variables. This matrix includes histograms for each variable along the diagonal, showcasing their distribution. The lower triangle displays scatter plots to explore the relationships between variable pairs, while the upper triangle shows the correlation coefficients, providing a quantitative measure of their relationships. This information allow us to identify the shape distribution, tendencies, outliers, among other things, thanks to different views in one grid.

```{r scatterPlot_contiuous, fig.align='center', message=FALSE, warning=FALSE}
# Select continuous variables (include target)
scatterPlot_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc", "class")]

# Create scatter plot matrix
ggpairs(scatterPlot_data,
        aes(fill="pink"),
        lower = list(continuous = "points", combo = "box_no_facet"),
        upper = list(continuous = "cor"),
        diag = list(continuous = "barDiag"),
        title = "Scatter plot matrix"
       )
```

**General insights**.

1. This analysis reveals a non-normal distribution for the numeric variables. To improve the data distribution for machine learning methods, such as *Neural networks*, the data will be transformed depending on its skewness. This below table shows the skewness measures for each variable and it has been obtained from `psych` [@psych_ref] package. Log-transformation are applied to variables with positive skewness (`bp`, `bgr`, `bu`, `sc`, `pot` and `wbcc`), while a square transformation is used for `sod` with negative skewness. Notice that the threshold for decision-making is set at skewness values above $1$ or below $-1$.

```{r skewness_analysis, warning=FALSE, message=FALSE, echo=FALSE}
numeric_data <- training_imp[, c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")]
skewness <- round(skew(numeric_data), 1)
result_df <- data.frame(variable = colnames(numeric_data), skewness = skewness)
pander(result_df)
```

2. Standardization of Data. The data will be standardized to benefit models that are sensitive to the scale of the data, such as *K-Nearest Neighbors* and *Neural Networks*. This step will not affect negatively to the other models like *Gradient Boosting*, which are less sensitive to scaling.

3. About correlation coefficients. It is worth mentioning that this dataset contains high correlation between some variables, this value is identify when coefficients are grater than $>0.7$ or lower than $< -0.7$). For instance, packed cell volume (`pcv`) and hemoglobin (`hemo`) have a strong positive correlation coefficient of $0.869$. From a biologically point of view makes sense, both parameter are related with the amount of oxygen going through human blood. Hemoglobin is the protein in red blood cells that carries oxygen, while the packed cell volume represents the volume percentage of red blood cells in blood. Moreover, red blood cells count (`rbcc`) is also highly correlated with `hemo` ($0.770$) and `pcv` ($0.766$) indicating that if one of this parameter increase the other one will increase too.

4. Outliers. It is notably the presence of some outliers over the variables. This observations (referring to the outliers) represent individuals with extreme values probably due to underlying medical conditions or measurement errors. Notice that this observations may influence directly the model performance and the conclusion derived from the data analysis.

**Relation with the output**. A deeper exploration of the most correlated variables with the target, `class`, let some important insights on identify chronic kidney disease and healthy patients. First pattern obtained is that people with chronic kidney disease (class = `ckd`) tend to have lower values of hemoglobin (`hemo`), packed cell volume (`pcv`), and red blood cell count (`rbcc`) as indicated by the red density distributions. In contrast, healthy patients (class = `notckd`) have higher values for these parameters, as shown by blue density distribution. Moreover, boxplots demonstrated same insight as before, with median values of `hemo`, `pcv`, and `rbcc` being lower in the chronic kidney disease group. Last distinction observed on the boxplots is about on how the outliers exist just on unhealthy people distribution. This situation might highlight the complexity for diagnosis and identifying the chronic kidney disease due to weird and unexpected behavior of this patients. 

```{r deeperExploration, warning=FALSE, message=FALSE, fig.align='center'}
deeperExploration_vars <- training_imp[, c("hemo", "pcv", "rbcc", "class")]

# Most correlate variables separate by target
ggpairs(deeperExploration_vars,
        aes(color = class, fill = class), # Apply color based on 'class'
        lower = list(continuous = wrap("points", alpha = 0.5)), # Transparency
        upper = list(continuous = "cor"),
        diag = list(continuous = wrap("densityDiag", alpha = 0.5))
) + 
  theme(legend.position = "bottom")
```

**Data transformation code**.

```{r numeric_transf}
# Numeric transformation
vars_log_transf <- c('bp', 'bgr', 'bu', 'sc', 'pot', 'wbcc') # Note: No zeros on any var

########################## TRAINING dataset ##########################
training_transf <- training_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
training_transf <- training_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
training_transf[,c("sod")] <- (training_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(training_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])

########################## TESTING dataset ##########################
testing_transf <- testing_imp
# Standardize the variables ----------------------------------------------------
# Log-transformation to solve right skewness
testing_transf <- testing_transf %>%
  mutate_at(vars(all_of(vars_log_transf)), ~ log(.))
# ^2-transformation to solve negative skewness
testing_transf[,c("sod")] <- (testing_transf[,c("sod")])^2
# Scale the variables ----------------------------------------------------------
testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")] <- scale(testing_transf[c("bp", "bgr", "bu", "sc", "sod", "pot", "hemo", "pcv", "wbcc", "rbcc")])
```

## Cualitative variables.

**Binary variables**. 

Some tables are display in order to understand better the dataset.
First table indicates that most of the patients have normal levels of red blood cells (`rbc`) and pus cell (`pc`). In the second table, the majority of data is regarding people without pus cell clumps (`pcc`) and bacteria (`ba`). The third table focus on various conditions, including hypertension (`htn`), diabetes mellitus (`dm`), coronary artery disease (`cad`), pedal edema (`pe`), and anemia (`ane`). Hypertension and diabetes mellitus seems as the more common conditions among the patients. Meanwhile the other parameters are present in no more than $19$% of the people tested, thus these conditions might not be commontly within the population. Finally, most people of the dataset report having a good appetite.
In summary, the binary variables are not equally distributed on this dataset.

```{r eda_binary_tables, echo=FALSE, warning=FALSE, message=FALSE}
# Tables for distributions -----------------------------------------------------
# Create tables depending on the possible output
summary_abnormal <- lapply(c("rbc", "pc"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_notpresent <- lapply(c("pcc", "ba"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_noYes <- lapply(c("htn", "dm", "cad", "pe", "ane"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)

summary_poor <- lapply(c("appet"), function(variable) {
  counts <- table(training_transf[[variable]])
  data.frame(
    Variable = variable,
    Category = names(counts),
    Count = as.integer(counts)
  )
}) %>%
  bind_rows() %>%
  pivot_wider(names_from = Variable, values_from = Count)


# Print tables
pander(summary_abnormal, format = "html", escape = FALSE)
pander(summary_notpresent, format = "html", escape = FALSE)
pander(summary_noYes, format = "html", escape = FALSE)
pander(summary_poor, format = "html", escape = FALSE)
```

The following grid of bar charts aim to show the target distribution across each binary variable. When looking at parameter related with infections, such as `ba` (bacteria) and `pcc` (pus cell clumps), these are less frequent in the overall dataset, but when they do occur, they are predominantly found in patients with kidney disease (class = `ckd`). In addition, poor appetite seems to be a common symptom in patients with ckd. Hypertension (`htn`) and diabetes mellitus (`dm`) are variables where storage a high number of sick patients with this conditions.

```{r eda_binary_plot, message=FALSE, warning=FALSE, echo=FALSE, fig.align='center'}
binary_vars <- c("rbc", "pc", "pcc", "ba", "htn", "dm", "cad", "appet", "pe", "ane")

# Bar plots by output ----------------------------------------------------------
# Reshape the df to long
long_data <- pivot_longer(training_transf, 
                          cols = binary_vars, 
                          names_to = "variable", 
                          values_to = "value")

# Combined plots with facet_wrap
p <- ggplot(long_data, aes(x = value, fill = as.factor(class))) + 
  geom_bar(position = "dodge", stat = "count") +
  facet_wrap(~variable, scales = "free_x") + 
  labs(y = 'Count', fill = 'Class') + 
  theme_minimal() + 
  theme(axis.text.x = element_text(hjust = 1),
        strip.text.x = element_text(face = "bold")) + 
  scale_fill_brewer(palette = "Set2")

print(p) # Display grid
```

**Category variables**.

Next step consist on explore the $3$ multi-state variables: specific gravity (`sg`), albumin (`al`), and sugar (`su`). Specific gravity clearly shows that levels $1.020$ and $1.025$ are the common levels for healthy people. Conversely, lower states are more prevalent among the presence of chronic kidney disease. On the other hand, healthy population is expect to have a level $0$ of albumin as well as a low level of sugar. Otherwise, levels indicates a higly link to this chronic disease.

```{r eda_category_distribution, echo=FALSE, fig.align='center'}
# Plot for Specific Gravity (sg)
plot_sg <- ggplot(training_transf, aes(x = as.factor(sg), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Specific Gravity by target",
       x = "Specific Gravity",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))

# Plot for Albumin (al)
plot_al <- ggplot(training_transf, aes(x = as.factor(al), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Albumin by target",
       x = "Albumin",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))  

# Plot for Sugar (su)
plot_su <- ggplot(training_transf, aes(x = as.factor(su), fill = class)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Sugar by target",
       x = "Sugar",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel2") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        plot.margin = margin(10, 10, 10, 10))

# Arrange plots in a grid
grid.arrange(plot_sg, plot_al, plot_su, ncol = 2)
```

Additionally, in order to confirm the above insights some stacked bar chart has been code. This plots shows the proportion of people with and without chronic kindey disease across the different categories of these three multi-state variables. All patterns are confirmed.

```{r eda_category_proportion, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
category_data <- melt(training_transf,
                      id.vars = "class",
                      measure.vars = c("sg", "al", "su"))

# Create the combined plot using the melted data
ggplot(category_data, aes(x = value, fill = class)) + 
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("notckd" = "darkolivegreen3", "ckd" = "brown3")) +
  labs(title = "Relationship with class",
       x = "", y = "Proportion") +
  facet_wrap(~variable, scales = "free_x", nrow = 2) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.background = element_blank(),
        strip.text.x = element_text(size = 10))
```

## Correlation matrix.

The heatmap represents the strength and direction of the relationships between the variables in the dataset, where the color intensity indicates the magnitude of the correlation. Dark colors, that can be either blue (positive) or red (negative), suggest a stronger relationship between variables. A positive coefficient means that an increase (or decrease) in one variable is associated with an increase (or decrease) in the other. Conversely, a negative correlation indicated that as one variables increase, the other decrease. And, color in the middle are interpreted as weak correlations.

For dealing with mixed data type, the `hetcor` function from the `psych` [@psych_ref] package in R is used. This enable the computation of correlation across numeric, binary and multi-state variables as long as the qualitative variable are represented in factors. When coding this step some inconveniences have been encountered with `al` and `htn` variables. To address this issue, both parameters are treated numerically to facilitate its computation in the correlation analysis.

```{r corr_heatmap, warning=FALSE, message=FALSE, fig.align='center'}
training_modify <- training_transf

# Some transformation needed for handling problematic variables: al, htn
training_modify$al <- as.numeric(as.character(training_modify$al))
#contrasts(training_transf$htn) # no as 0 and yes as 1
training_modify$htn <- as.numeric(training_modify$htn)

# Works for mix variables: binary, multi-state and numeric
# Qualitative variables have to be in factor
cor_matrix <- hetcor(training_modify)
cor_values <- cor_matrix$correlations
corrplot(cor_values, method="color", type="upper", order="hclust",
         tl.cex=0.7, tl.col="black", tl.srt=45)
```

**Analytical exploration**.

Thanks to this exploration $17$ strong relationships have been discovered (the threshold for filter the correlations is set to coefficients higher than $0.7$ or lower than $-0.7$). However, I decided to focus on the top $8$ with the most significant impact, which can seen on the below table.

Most of this relationships are kind of intuitive. For example, having diabetes is closely linked to higher sugar levels, and a lower amount of hemoglobin seems to be associated with anemia. Overall, it is realised how related metabolomic and blood measures are when identifying chronic kidney disease,

```{r}
cor_flat <- as.data.frame(as.table(cor_values))
cor_flat <- cor_flat[order(-abs(cor_flat$Freq)),]

# Filter correlations: >0.7 or <-0.7
strong_correlations <- subset(cor_flat, abs(Freq) > 0.7 & Var1 != Var2)

# Remove duplicates rows
strong_correlations$pairID <- apply(strong_correlations[, c("Var1", "Var2")], 
                                    1,
                                    function(x) paste(sort(x), collapse = "-"))
strong_correlations <- strong_correlations[!duplicated(strong_correlations$pairID),]
strong_correlations$pairID <- NULL # Remove ID

# Add sign
strong_correlations$Sign <- ifelse(strong_correlations$Freq > 0, "+", "-")

#pander(strong_correlations) # 17 high realtionship
head(strong_correlations, 8) # Display the highest 8 relations
```

## Target variable.

To see the distribution of our target variable (`class`), we first look at the raw counts and then at the percentages to understand the proportion of sickness patients. To conclude, it is worth mentioning that the output varibale is not imbalanced, thereby there is no need for upsampling or downsampling this parameter.

```{r target_distribution}
# Check counts of chronic kidney disease
pander(table(data$class))

# Results in percentage
pander(prop.table(table(data$class)) * 100)
```

\pagebreak
# Machine-learning tools

For this section are defined some common consideration that I will assume for all models implemented.

1º. The dataset is divided into predictors and the target variable.

```{r split_subsets}
# Divide into predictors and target both subsets
X_train <- training_transf %>% dplyr::select(-class)
y_train <- training_transf$class

X_test <- testing_transf %>% dplyr::select(-class)
y_test <- testing_transf$class
n_test <- dim(testing_transf)[1]
```

2º. Control to optimize the hyper-parameters.

```{r}
# Set up train control:
# cross-validation -> 5 folds repeated 10-times
# "summaryFunction = defaultSummary" -> Compute accuracy and kappa statistic
ctrl <- trainControl(method = "repeatedcv",
                     #summaryFunction = defaultSummary,
                     number = 5, 
                     repeats = 10,
                     verboseIter = TRUE, # To track cv
                     classProbs = TRUE)
```

**REVISAR ESTO!**

3º. Same procedure has been conducted over all machine learning tools. First, it has been done a broad search of the hyper-parameter, I analyze the results to identify a more narrow range of these parameters values where performance seems to optimize, and then conduct a more focused search within that range. On this sections, the script shows the final hyper-parameter setting (narrow search), some comments can guide the reader to understand the selection of these values. Also, this narrow search allow the document to be less time-consuming.

And important step always is to take a look of the parameters we can handle, when using the caret library.

```{r caret_exploration_models, eval=FALSE}
# Models supported by caret
names(getModelInfo())

# In order to explore which parameter can be tune
name_caret_model <- "knn" # Change me!
modelLookup(name_caret_model)
```


```{r folder_pretrain_mod, eval=FALSE}
# Specify the folder path
folder_path <- "pretrained_models"
# Create the folder if it doesn't exist
if (!dir.exists(folder_path)) {
  dir.create(folder_path)
}
```

```{r eval=FALSE, include=FALSE}
# KNN model --------------------------------------------------------------------
knn_fit <- readRDS("pretrained_models/knn_fit.rds")
# SVM model --------------------------------------------------------------------
svm_fit <- readRDS("pretrained_models/svm_fit.rds")
# DT model ---------------------------------------------------------------------
rpart_fit <- readRDS("pretrained_models/rpart_fit.rds")
c50_fit <- readRDS("pretrained_models/c50_fit.rds")
# RF model ---------------------------------------------------------------------
rf_fit_final <- readRDS("pretrained_models/rf_fit_final.rds")
# GB model ---------------------------------------------------------------------

# NN model ---------------------------------------------------------------------
nn_fit <- readRDS("pretrained_models/nn_fit.rds")
# DNN model ---------------------------------------------------------------------
dnn_fit <- readRDS("pretrained_models/dnn_fit.rds")
```

## k-Nearest Neighbors

```{r}
# Hyper-parameter search -------------------------------------------------------
# K no more than 50% of the training set size.
# From lecture: preferable to try odd numbers for k
knn_grid <- expand.grid(k = seq(3, 121, by = 2))

# Training the model -----------------------------------------------------------
knn_fit <- train(class ~ .,
                method = "knn",
                data = training_transf,
                tuneGrid = knn_grid,
                metric = "Accuracy",
                trControl = ctrl)
print(knn_fit)
# Best values: k = 3

# Save model -------------------------------------------------------------------
saveRDS(knn_fit, file="pretrained_models/knn_fit.rds")

# Predictions on test ----------------------------------------------------------
knn_pred = predict(knn_fit, X_test)
knn_pred_prob = predict(knn_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(knn_pred, y_test)
```

**Performance of the model**

TESTING PERFORMANCE

- Purpose: To compare the predicted probabilities against the actual classes (true labels).

- Insight: Ideally, you would want to see a clear separation where most of the "ckd" actual class points have high predicted probabilities, and the "notckd" points have lower predicted probabilities. Overlap indicates misclassification. If the "ckd" points are spread across a wide range of probabilities, it indicates that the model has difficulty distinguishing between classes.

```{r knn_measures, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot tuning hyper-parameter --------------------------------------------------
knn_results_search <- knn_fit$results

# df to long format
knn_results_long <- melt(knn_results_search,
                     id.vars = "k", 
                     measure.vars = c("Accuracy", "Kappa"),
                     variable.name = "Metric", value.name = "Value")

# Plotting both metrics
ggplot(knn_results_long, aes(x = k, y = Value, color = Metric)) + 
  geom_line() + 
  geom_point() +
  scale_color_manual(values = c("Accuracy" = "dodgerblue4", "Kappa" = "darkorange3")) +
  xlab("Number of neighbors (k)") + 
  ylab("Metric value") +
  ggtitle("KNN tuning vs. Number of neighbors") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank()) +
  theme(legend.position = "bottom")

# Plot importance on training --------------------------------------------------
knn_var_imp <- varImp(knn_fit, scale=FALSE)
plot(knn_var_imp, main="Variable importance for KNN")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
kkn_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = knn_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = kkn_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with KNN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_knn <- roc(y_test, knn_pred_prob[,2])
plot(roc_knn, main="ROC Curve for Optimal KNN Model")
```

## Support Vector Machines

Well know as SVM, the idea is to maximize the gap between the two classes. This models works really nice for non-linear problems. The original sample is transformed into a higher space (using kernels: lineal, polinomial, gaussian (radial-basis), two-layer perceptron, etc.).
Some advantages that make it think it can work properly for this dataset is that SVM is not influenced by outlier (that we have some) and does no make overfitting.

```{r}
# Hyper-parameter search -------------------------------------------------------
# C; compromising between low errors and weights values
# sigma; determine how far the influence of a single training example reaches.
svm_grid <- expand.grid(C =  c(1e-1, 1e0, 1e1, 1e2, 1e3, 1e4), 
                        sigma = c(0.01, 0.05, 0.1, 0.5, 1, 2, 5)
                        )

# Training the model -----------------------------------------------------------
svm_fit <- train(class ~., method = "svmRadial", 
                data = training_transf,
                tuneGrid = svm_grid, 
                metric = "Accuracy",
                trControl = ctrl)

print(svm_fit)
# Best values: sigma = 0.5 and C = 0.1

# Save model -------------------------------------------------------------------
saveRDS(svm_fit, file="pretrained_models/svm_fit.rds")

# Predictions -----------------------------------------------------------------
svm_pred = predict(svm_fit, X_test)
svm_pred_prob = predict(svm_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(svm_pred, y_test)
```

**Performance of the model**

```{r svm_measures, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot tuning hyper-parameter --------------------------------------------------
selected_sigma_values <- unique(svm_fit$results$sigma)
svm_results_filtered <- subset(svm_fit$results, sigma %in% selected_sigma_values)

ggplot(svm_results_filtered, aes(x=log10(C), y=Accuracy, color=factor(sigma))) + 
  geom_line() + 
  geom_point() +
  xlab("log10(C)") + 
  ylab("Accuracy") +
  ggtitle("Accuracy vs. C for selected sigma Values") +
  scale_color_discrete(name = "sigma") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank())

# Plot importance on training --------------------------------------------------
svm_var_imp <- varImp(svm_fit, scale=FALSE)
plot(svm_var_imp, main="Variable importance for SVM")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
svm_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = svm_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = svm_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with KNN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_svm <- roc(y_test, svm_pred_prob[,2])
plot(roc_svm, main="ROC Curve for Optimal KNN Model")
```

## Decision trees, Random Forests, Gradient Boosting

### Decision trees

In general, some advantages are: interpretability, automatic feature selection, less data cleaning (NAs and outliers). There is no existance of missing values, but there are some outliers this model can actually works really nice.

**rpart model**

Limited DT model, it does not support boosting option. 

```{r}
# Hyper-parameter search -------------------------------------------------------
# cp; size of the tree
rpart_grid <- expand.grid(.cp = c(0.01, 0.05, 0.1))

# Training the model -----------------------------------------------------------
rpart_fit <- train(class ~., 
                 method = "rpart",
                 data = training_transf,
                 tuneGrid = rpart_grid,
                 metric = "Accuracy",
                 trControl = ctrl)

print(rpart_fit)
# Best values: cp = 0.01

# Save model -------------------------------------------------------------------
saveRDS(rpart_fit, file="pretrained_models/rpart_fit.rds")

# Predictions ------------------------------------------------------------------
rpart_pred = predict(rpart_fit, X_test)
rpart_pred_prob = predict(rpart_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(rpart_pred, y_test)
```

```{r warning=FALSE, message=FALSE}
# DT visualization -------------------------------------------------------------
# REVIEW: Choose the second option, it is more visually appeling
# plot the model
plot(rpart_fit$finalModel, uniform=TRUE,
     main="Classification Decision Tree")
text(rpart_fit$finalModel, use.n.=TRUE, all=TRUE, cex=.8)

suppressMessages(library(rattle))
fancyRpartPlot(rpart_fit$finalModel)

# Plot tuning hyper-parameter --------------------------------------------------
plot(rpart_fit, main = 'Hyper-parameter tuning - Decision tree')

# Plot importance on training --------------------------------------------------
rpart_var_imp <- varImp(rpart_fit, scale=FALSE)
plot(rpart_var_imp, main="Variable importance for Decision Tree")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
rpart_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = rpart_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = rpart_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with rpart") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_rpart <- roc(y_test, rpart_pred_prob[,2])
plot(roc_rpart, main="ROC Curve for Optimal KNN Model")
```

**Tree model**

Advanced DT model, somehow it's a boosting approach 

```{r warning=FALSE, message=FALSE}
# Hyper-parameter search -------------------------------------------------------
# .winnow; feature selection step conducted before modelling
# .trial; number of boosting iterations
c50_grid <- expand.grid( .winnow = c(TRUE, FALSE), 
                         .trials=c(1, 5, 4),
                         .model= "tree"
                         )

# Training the model -----------------------------------------------------------
c50_fit <- train(class ~., 
                 method = "C5.0",
                 data = training_transf,
                 tuneGrid = c50_grid,
                 metric = "Accuracy",
                 trControl = ctrl)

print(c50_fit)
# Best values: trials = 1, model = tree and winnow = FALSE.

# Save model -------------------------------------------------------------------
saveRDS(c50_fit, file="pretrained_models/c50_fit.rds")

# Predictions -----------------------------------------------------------------
c50_pred = predict(c50_fit, X_test)
c50_pred_prob = predict(c50_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(c50_pred, y_test)
```

```{r}
# Plot tuning hyper-parameter --------------------------------------------------
plot(c50_fit, main = 'Hyper-parameter tuning - Decision tree')

# Plot importance on training --------------------------------------------------
c50_var_imp <- varImp(c50_fit, scale=FALSE)
plot(c50_var_imp, main="Variable importance for Decision Tree")

# DT visualization -------------------------------------------------------------
# TODO: implement this?

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
c50_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = c50_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = c50_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with C.50") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_c50 <- roc(y_test, svm_pred_prob[,2])
plot(roc_c50, main="ROC Curve for Optimal KNN Model")
```

### Random Forests

Random forest are a more advanced tools of DTs
It also reduces overtting: for each node of the tree, consider only a random subset of predictors, m < p, for instance m = sqrt(p).

How it works?
1. Draw B bootstrap samples of size n: a decision tree for each bootstrap sample
2. Grow each tree, by selecting a random set of m (out of p) features at each node, and choosing the best feature to split on
3. Regression: Aggregate the predictions of the trees to produce the final prediction.

Thus, `caret` library only allow to tune the `mtry` parameter I want to illustrate that it is possible to search for other parameters by coding manually the search. On this model, the search has been expanded for `ntree` parameter.

```{r}
# Hyper-parameter search -------------------------------------------------------
# mtry; nº of variables randomly choose for bulding the trees
# Common practice for the grid is [1, sqrt(n_variables)]
rf_grid <- expand.grid(mtry = c(1, 2, 3, 4, 5))

# Training the model -----------------------------------------------------------
rf_fit <- train(class ~.,
                method = "rf",
                data = training_transf,
                ntree = 200,
                tuneGrid = rf_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(rf_fit)
# Best values: mtry = 2

# Try different ntree values manually ------------------------------------------
model_rf_list <- list()
testing_ntree_var <- seq(200, 1000, by = 100)

for (ntree in testing_ntree_var){
  fit <- train(class~.,
               method = 'rf',
               data = training_transf,
               ntree = ntree,
               tuneGrid = rf_grid,
               metric = 'Accuracy',
               trControl = ctrl)
  key <- toString(ntree)
  model_rf_list[[key]] <- fit
}

results_rf_manually <- resamples(model_rf_list)
#summary(results_rf_manually)
# Best values: ntree = 1000
dotplot(results_rf_manually)

# Final search with ntree=1000
rf_fit_final <- train(class ~.,
                method = "rf",
                data = training_transf,
                ntree = 1000,
                tuneGrid = rf_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(rf_fit_final)

# Save model -------------------------------------------------------------------
saveRDS(rf_fit_final, file="pretrained_models/rf_fit_final.rds")

# Predictions ------------------------------------------------------------------
rf_predF = predict(rf_fit_final, X_test)
rf_predF_prob = predict(rf_fit_final, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(rf_predF, y_test)
```

library(pdp) this library allow us to see the dependencies/Relation between variables. For example, we see with the first line of code that the dependency is not-lineal.
In LR the relation was a line, and it was capable to capture this relation.

```{r warning=FALSE, message=FALSE}
# Plot tuning hyper-parameter --------------------------------------------------
plot(rf_fit_final, main = 'Hyper-parameter tuning - Random Forest')

# Plot importance on training --------------------------------------------------
rf_var_imp <- varImp(rf_fit_final, scale=FALSE)
plot(rf_var_imp, main="Variable importance for Random Forest")

# Explore the relationship of the most important variable
library(pdp)
partial(rf_fit_final, pred.var = "hemo", which.class=2, plot = TRUE, prob=TRUE, rug = TRUE)
# Se pueden analizar más

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
rf_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = rf_predF_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = rf_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with RF") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_rf <- roc(y_test, rf_predF_prob[,2])
plot(roc_rf, main="ROC Curve for Optimal RF Model")
```

### Gradient Boosting

```{r}
# Hyper-parameter search -------------------------------------------------------
#https://csantill.github.io/RTuningModelParameters/
xgb_grid = expand.grid(
  nrounds = c(100, 500),
  eta = c(0.01, 0.1), # Learning rate
  max_depth = c(3, 6),  # Tree depth
  gamma = c(0, 1),
  colsample_bytree = c(0.5, 0.8), # Feature sampling
  min_child_weight = c(1, 5),
  subsample = c(0.7, 0.9) 
)
# dim(xgb_grid) # 128 combinaciones 
# Training the model -----------------------------------------------------------
xgb_fit = train(class ~.,
                data = training_transf,
                tuneGrid = xgb_grid,
                metric = "Accuracy",
                method = "xgbTree",
                trControl = ctrl
                )

print(xgb_fit)
# Best values: Fitting nrounds = 500, max_depth = 6, eta = 0.01, gamma = 0, colsample_bytree = 0.5, min_child_weight = 1, subsample = 0.9 on full training set

# Predictions -----------------------------------------------------------------
xgb_pred = predict(xgb_fit, X_test)
xgb_pred_prob = predict(xgb_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(xgb_pred, y_test)
```

## Neural Networks

```{r warning=FALSE, message=FALSE}
# Hyper-parameter search -------------------------------------------------------
# size; nº of neurons in the hidden layer
# decay
nn_grid <- expand.grid(size=c(2,4,6), 
                       decay=c(0.01,0.001))

# Training the model -----------------------------------------------------------
# NN with 1 hidden layer
nn_fit <- train(class ~.,
                method = "nnet",
                data = training_transf,
                MaxNWts = 1000,
                maxit = 100,
                tuneGrid = nn_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(nn_fit)
# Best values: size = 6 and decay = 0.001

# Save model -------------------------------------------------------------------
saveRDS(nn_fit, file="pretrained_models/nn_fit.rds")

# Predictions -----------------------------------------------------------------
nn_pred = predict(nn_fit, X_test)
nn_pred_prob = predict(nn_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(nn_pred, y_test)
```

```{r nn_measures, warning=FALSE, message=FALSE, fig.align='center', out.width="80%"}
# Plot tuning hyper-parameter --------------------------------------------------
plot(nn_fit)

# Plot importance on training --------------------------------------------------
nn_imp <- varImp(nn_fit, scale = F)
plot(nn_imp,
     scales = list(y = list(cex = .95)), 
     main="Variable importance for Neural Networks")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
nn_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = nn_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = nn_test_plot, aes(x = Individual, 
                                 y = Probability_CKD, 
                                 color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with NN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_nn <- roc(y_test, nn_pred_prob[,2])
plot(roc_nn, main="ROC Curve for Optimal NN Model")
```

## Deep Neural Network

```{r warning=FALSE, message=FALSE}
# Hyper-parameter search -------------------------------------------------------
# layer1, layer2, layer3: nº of neuron in each hidden layers
dnn_grid <- expand.grid(layer1 = c(5, 10, 15),
                        layer2 = c(0, 5, 10),
                        layer3 = c(0, 5),
                        hidden_dropout = c(0.1, 0.2), #dense neuron network
                        visible_dropout = c(0.1, 0.2))
# dim(dnn_grid) # 72 combinations

# Training the model -----------------------------------------------------------
# NN with 1 hidden layer
dnn_fit <- train(class ~.,
                method = "dnn",
                data = training_transf,
                numepochs = 20, # nº of iterations on the whole training set
                tuneGrid = dnn_grid,
                metric = "Accuracy",
                trControl = ctrl)

print(dnn_fit)
# Best values:

# Save model -------------------------------------------------------------------
saveRDS(dnn_fit, file="pretrained_models/dnn_fit.rds")

# Predictions -----------------------------------------------------------------
dnn_pred = predict(dnn_fit, X_test)
dnn_pred_prob = predict(dnn_fit, X_test, type = "prob")
# REVIEW: With prob predictions, we can vary the threshold

# Performance measures ---------------------------------------------------------
confusionMatrix(dnn_pred, y_test)
```

```{r}
# Plot tuning hyper-parameter --------------------------------------------------
plot(dnn_fit)

# Plot importance on training --------------------------------------------------
dnn_var_imp <- varImp(dnn_fit, scale = FALSE)
plot(dnn_var_imp,
     scales = list(y = list(cex = .95)), 
     main="Variable importance for Deep Neural Netwrok")

# Plot testing performance -----------------------------------------------------
# Convert the test set and predictions into a suitable dataframe for ggplot
dnn_test_plot <- data.frame(
  Individual = seq_along(y_test),
  Probability_CKD = dnn_pred_prob[,2],
  Actual_Class = as.factor(y_test)
)

# Create the plot
ggplot(data = dnn_test_plot, 
       aes(x = Individual, y = Probability_CKD, color = Actual_Class)) +
  geom_point() +
  theme_light(base_size = 14) +
  scale_color_manual(values = c("notckd" = "deepskyblue2", "ckd" = "firebrick2")) +
  xlab("Test observation") +
  ylab("Probability of class with DNN") +
  theme_minimal() +
  geom_hline(yintercept = 0.5, linetype = "dashed")

# ROC --------------------------------------------------------------------------
roc_dnn <- roc(y_test, dnn_pred_prob[,2])
plot(roc_dnn, main="ROC Curve for Optimal DNN Model")
```

# Overview models

```{r}
# TODO: Add GB model

# Compare model performance ----------------------------------------------------
models_compare <- resamples(list(KNN = knn_fit,
                                 SVM = svm_fit,
                                 DT.rpart = rpart_fit,
                                 DT.c50 = c50_fit,
                                 RF = rf_fit_final,
                                 NN = nn_fit,
                                 DNN = dnn_fit)
                            )

# Summary
summary(models_compare)

# Box-and-Whisker Plot
bwplot(models_compare, metric = "Accuracy",
       main = "Model comparison based on Accuracy")

# Dotplot
dotplot(models_compare, metric = "Accuracy",
        main = "Model Comparison based on Accuracy")
```

\pagebreak
# Social impact

2º. **Economic assumption used**. Incorporating a custom metric in Caret.

- If the doctor correctly diagnose a patience does not have a chronic kidney disease, the 

**Economic assumption used**.

- If the company predicts a customer is going to stay and it actually stay, the company gains a 12% profit at the end of the quarter.

- If the company predicts a customer is going to stay but he leaves, the loss is 100%.

- If the company predict a customer is going to leave but he actually stay, the profit is 10%.

- If the company predict a customer is going to leave and he actually leave, then the profit is 3%.

Using the specified profits and losses for each prediction outcome, we compute the average profit per customer based on the model's predictions.

```{r eval=FALSE}
# lev=NULL is mandatory to get the k. So caret can interprretate this is the accuracy (or somthing like that ha dicho)
profit_unit <- c(0.12, -0.01, -1.0, 0.0)
EconomicProfit <- function(data, lev = NULL, model = NULL) 
{
  y_pred = data$pred 
  y_true = data$obs
  CM = confusionMatrix(y_pred, y_true)$table
  out = sum(profit_unit*CM)/sum(CM)
  names(out) <- c("EconomicProfit")
  out
}
```

\pagebreak
# References

<div id="refs"></div>
